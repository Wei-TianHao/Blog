<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>元学习: 学习如何学习</title>
    <meta name="description" content="学习如何学习的方法被称为元学习。元学习的目标是在接触到没见过的任务或者迁移到新环境中时，可以根据之前的经验和少量的样本快速学习如何应对。元学习有三种常见的实现方法：1）学习有效的距离度量方式（基于度量的方法）；2）使用带有显式或隐式记忆储存的（循环）神经网络（基于模型的方法）；3）训练以快速学习为目标的模型（基于...">

    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/2019/09/16/meta-learning.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-8161570-6', 'auto');
        ga('send', 'pageview');
    </script>

    <!-- For Facebook share button -->
    <div id="fb-root"></div>
    <script>
      (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));
    </script>

</head>


  <body>

    <header class="site-header" role="banner">

    <div class="wrapper">
        
        <a class="site-title" href="/">Tianhao</a>

        <nav class="site-nav">
            <a class="page-link" href="http://Wei-Tianhao.github.io" target="_blank">&#x1f349; About</a>
        </nav>

        <nav class="site-nav">
            <a class="page-link" href="/contact.html">&#x1f4ee; Contact</a>
        </nav>

    </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">元学习: 学习如何学习</h1>
    <p class="post-meta">

      <time datetime="2019-09-16T20:00:00-04:00" itemprop="datePublished">
        
        Sep 16, 2019
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Lilian Weng</span>
      </span>

      <span>
        
          
          <a class="post-tag" href="/tag/meta-learning"><nobr>meta-learning</nobr>&nbsp;</a>
        
          
          <a class="post-tag" href="/tag/long-read"><nobr>long-read</nobr>&nbsp;</a>
        
      </span>
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2019/09/16/meta-learning.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>学习如何学习的方法被称为元学习。元学习的目标是在接触到没见过的任务或者迁移到新环境中时，可以根据之前的经验和少量的样本快速学习如何应对。元学习有三种常见的实现方法：1）学习有效的距离度量方式（基于度量的方法）；2）使用带有显式或隐式记忆储存的（循环）神经网络（基于模型的方法）；3）训练以快速学习为目标的模型（基于优化的方法）</p>
</blockquote>

<!--more-->

<p>好的机器学习模型经常需要大量的数据来进行训练，但人却恰恰相反。小孩子看过一两次喵喵和小鸟后就能分辨出他们的区别。会骑自行车的人很快就能学会骑摩托车，有时候甚至不用人教。那么有没有可能让机器学习模型也具有相似的性质呢？如何才能让模型仅仅用少量的数据就学会新的概念和技能呢？这就是<strong>元学习</strong>要解决的问题。</p>

<p>我们期望好的元学习模型能够具备强大的适应能力和泛化能力。在测试时，模型会先经过一个自适应环节（adaptation process），即根据少量样本学习任务。经过自适应后，模型即可完成新的任务。自适应本质上来说就是一个短暂的学习过程，这就是为什么元学习也被称作<a href="https://www.cs.cmu.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf">“学习”学习</a>.</p>

<p>元学习可以解决的任务可以是任意一类定义好的机器学习任务，像是监督学习，强化学习等。具体的元学习任务例子有：</p>
<ul>
  <li>在没有猫的训练集上训练出来一个图片分类器，这个分类器需要在看过少数几张猫的照片后分辨出测试集的照片中有没有猫。</li>
  <li>训练一个玩游戏的AI，这个AI需要快速学会如何玩一个从来没玩过的游戏。</li>
  <li>一个仅在平地上训练过的机器人，需要在山坡上完成给定的任务。</li>
</ul>

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#元学习问题定义" id="markdown-toc-元学习问题定义">元学习问题定义</a>    <ul>
      <li><a href="#a-simple-view" id="markdown-toc-a-simple-view">A Simple View</a></li>
      <li><a href="#像测试一样训练" id="markdown-toc-像测试一样训练">像测试一样训练</a></li>
      <li><a href="#学习器和元学习器" id="markdown-toc-学习器和元学习器">学习器和元学习器</a></li>
      <li><a href="#常见方法" id="markdown-toc-常见方法">常见方法</a></li>
    </ul>
  </li>
  <li><a href="#metric-based" id="markdown-toc-metric-based">Metric-Based</a>    <ul>
      <li><a href="#convolutional-siamese-neural-network" id="markdown-toc-convolutional-siamese-neural-network">Convolutional Siamese Neural Network</a></li>
      <li><a href="#matching-networks" id="markdown-toc-matching-networks">Matching Networks</a>        <ul>
          <li><a href="#simple-embedding" id="markdown-toc-simple-embedding">Simple Embedding</a></li>
          <li><a href="#full-context-embeddings" id="markdown-toc-full-context-embeddings">Full Context Embeddings</a></li>
        </ul>
      </li>
      <li><a href="#relation-network" id="markdown-toc-relation-network">Relation Network</a></li>
      <li><a href="#prototypical-networks" id="markdown-toc-prototypical-networks">Prototypical Networks</a></li>
    </ul>
  </li>
  <li><a href="#model-based" id="markdown-toc-model-based">Model-Based</a>    <ul>
      <li><a href="#memory-augmented-neural-networks" id="markdown-toc-memory-augmented-neural-networks">Memory-Augmented Neural Networks</a>        <ul>
          <li><a href="#mann-for-meta-learning" id="markdown-toc-mann-for-meta-learning">MANN for Meta-Learning</a></li>
          <li><a href="#addressing-mechanism-for-meta-learning" id="markdown-toc-addressing-mechanism-for-meta-learning">Addressing Mechanism for Meta-Learning</a></li>
        </ul>
      </li>
      <li><a href="#meta-networks" id="markdown-toc-meta-networks">Meta Networks</a>        <ul>
          <li><a href="#fast-weights" id="markdown-toc-fast-weights">Fast Weights</a></li>
          <li><a href="#model-components" id="markdown-toc-model-components">Model Components</a></li>
          <li><a href="#training-process" id="markdown-toc-training-process">Training Process</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#optimization-based" id="markdown-toc-optimization-based">Optimization-Based</a>    <ul>
      <li><a href="#lstm-meta-learner" id="markdown-toc-lstm-meta-learner">LSTM Meta-Learner</a>        <ul>
          <li><a href="#why-lstm" id="markdown-toc-why-lstm">Why LSTM?</a></li>
          <li><a href="#model-setup" id="markdown-toc-model-setup">Model Setup</a></li>
        </ul>
      </li>
      <li><a href="#maml" id="markdown-toc-maml">MAML</a>        <ul>
          <li><a href="#first-order-maml" id="markdown-toc-first-order-maml">First-Order MAML</a></li>
        </ul>
      </li>
      <li><a href="#reptile" id="markdown-toc-reptile">Reptile</a>        <ul>
          <li><a href="#the-optimization-assumption" id="markdown-toc-the-optimization-assumption">The Optimization Assumption</a></li>
          <li><a href="#reptile-vs-fomaml" id="markdown-toc-reptile-vs-fomaml">Reptile vs FOMAML</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="元学习问题定义">元学习问题定义</h2>

<p>在本文中，我们主要关注监督学习中的元学习任务，比如图像分类。在之后的文章中我们会继续讲解更有意思的元强化学习。</p>

<h3 id="a-simple-view">A Simple View</h3>

<p>我们现在假设有一个任务的分布，我们从这个分布中采样了许多任务作为训练集。好的元学习模型在这个训练集上训练后，应当对这个空间里所有的任务都具有良好的表现，即使是从来没见过的任务。每个任务可以表示为一个数据集<script type="math/tex">\mathcal{D}</script>，数据集中包括特征向量<script type="math/tex">x</script>和标签<script type="math/tex">y</script>，分布表示为<script type="math/tex">p(\mathcal{D})</script>。那么最佳的元学习模型参数可以表示为：</p>

<script type="math/tex; mode=display">\theta^* = \arg\min_\theta \mathbb{E}_{\mathcal{D}\sim p(\mathcal{D})} [\mathcal{L}_\theta(\mathcal{D})]</script>

<p>上式的形式跟一般的学习任务非常像，只不过上式中的每个<em>数据集</em>是一个<em>数据样本</em>。</p>

<p><em>少样本学习（Few-shot classification）</em> 是元学习的在监督学习中的一个实例。数据集<script type="math/tex">\mathcal{D}</script>经常被划分为两部分，一个用于学习的支持集（support set）<script type="math/tex">S</script>，和一个用于训练和测试的预测集（prediction set）<script type="math/tex">B</script>，即<script type="math/tex">\mathcal{D}=\langle S, B\rangle</script>。<em>K-shot N-class</em>分类任务，即支持集中有N类数据，每类数据有K个带有标注的样本。</p>

<p><img src="../assets/images/2019-09-19-meta-learning/few-shot-classification.png" alt="few-shot-classification" />
<em>Fig. 1. 4-shot 2-class 图像分类的例子。 (图像来自<a href="https://www.pinterest.com/">Pinterest</a>)</em></p>

<h3 id="像测试一样训练">像测试一样训练</h3>

<p>一个数据集<script type="math/tex">\mathcal{D}</script>包含许多对特征向量和标签，即<script type="math/tex">\mathcal{D} = \{(\mathbf{x}_i, y_i)\}</script>。每个标签属于一个标签类<script type="math/tex">\mathcal{L}</script>。假设我们的分类器<script type="math/tex">f_\theta</script>的输入是特征向量<script type="math/tex">\mathbf{x}</script>，输出是属于第<script type="math/tex">y</script>类的概率<script type="math/tex">P_\theta(y\vert\mathbf{x})</script>，<script type="math/tex">\theta</script>是分类器的参数。</p>

<p>如果我们每次选一个<script type="math/tex">B \subset \mathcal{D}</script>作为训练的batch，则最佳的模型参数，应当能够最大化，多组batch的正确标签概率之和。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta^* &= {\arg\max}_{\theta} \mathbb{E}_{(\mathbf{x}, y)\in \mathcal{D}}[P_\theta(y \vert \mathbf{x})] &\\
\theta^* &= {\arg\max}_{\theta} \mathbb{E}_{B\subset \mathcal{D}}[\sum_{(\mathbf{x}, y)\in B}P_\theta(y \vert \mathbf{x})] & \scriptstyle{\text{; trained with mini-batches.}}
\end{aligned} %]]></script>

<p>few-shot classification的目标是，在小规模的support set上“快速学习”（类似fine-tuning）后，能够减少在prediction set上的预测误差。为了训练模型快速学习的能力，我们在训练的时候按照以下步骤：</p>
<ol>
  <li>采样一个标签的子集, <script type="math/tex">L\subset\mathcal{L}</script>.</li>
  <li>根据采样的标签子集，采样一个support set <script type="math/tex">S^L \subset \mathcal{D}</script> 和一个training batch <script type="math/tex">B^L \subset \mathcal{D}</script>。<script type="math/tex">S^L</script>和<script type="math/tex">B^L</script>中的数据的标签都属于<script type="math/tex">L</script>，即<script type="math/tex">y \in L, \forall (x, y) \in S^L, B^L</script>.</li>
  <li>把support set作为模型的输入，进行“快速学习”。注意，不同的算法具有不同的学习策略，但总的来说，这一步不会永久性更新模型参数。 <!-- , $$\hat{y}=f_\theta(\mathbf{x}, S^L)$$ --></li>
  <li>把prediction set作为模型的输入，计算模型在<script type="math/tex">B^L</script>上的loss，根据这个loss进行反向传播更新模型参数。这一步与监督学习一致。</li>
</ol>

<p>你可以把每一对<script type="math/tex">(S^L, B^L)</script>看做是一个数据点。模型被训练出了在其他数据集上扩展的能力。下式中的红色部分是元学习的目标相比于监督学习的目标多出来的部分。</p>

<script type="math/tex; mode=display">\theta = \arg\max_\theta \color{red}{E_{L\subset\mathcal{L}}[} E_{\color{red}{S^L \subset\mathcal{D}, }B^L \subset\mathcal{D}} [\sum_{(x, y)\in B^L} P_\theta(x, y\color{red}{, S^L})] \color{red}{]}</script>

<p>这个想法有点像是我们面对某个只有少量数据的任务时，会使用在相关任务的大数据集上预训练的模型，然后进行fine-tuning。像是图形语义分割网络可以用在ImageNet上预训练的模型做初始化。相比于在一个特定任务上fine-tuning使得模型更好的拟合这个任务，元学习更进一步，它的目标是让模型优化以后能够在多个任务上表现的更好，类似于变得更容易被fine-tuning。</p>

<h3 id="学习器和元学习器">学习器和元学习器</h3>

<p>还有一种常见的看待meta-learning的视角，把模型的更新划分为了两个阶段：</p>
<ul>
  <li>根据给定的任务，训练一个分类器<script type="math/tex">f_\theta</script>完成任务，作为“学习器”模型</li>
  <li>同时，训练一个元学习器<script type="math/tex">g_\phi</script>，根据support set <script type="math/tex">S</script>学习如何更新学习器模型的参数。<script type="math/tex">\theta' = g_\phi(\theta, S)</script></li>
</ul>

<p>则最后的优化目标中，我们需要更新<script type="math/tex">\theta</script>和<script type="math/tex">\phi</script>来最大化：</p>

<script type="math/tex; mode=display">\mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]]</script>

<h3 id="常见方法">常见方法</h3>

<p>元学习主要有三类常见的方法：metric-based，model-based，optimization-based。
Oriol Vinyals在meta-learning symposium @ NIPS 2018上做了一个很好的<a href="http://metalearning-symposium.ml/files/vinyals.pdf">总结</a>：</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Model-based</th>
      <th>Metric-based</th>
      <th>Optimization-based</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Key idea</strong></td>
      <td>RNN; memory</td>
      <td>Metric learning</td>
      <td>Gradient descent</td>
    </tr>
    <tr>
      <td><strong>How <script type="math/tex">P_\theta(y \vert \mathbf{x})</script> is modeled?</strong></td>
      <td><script type="math/tex">f_\theta(\mathbf{x}, S)</script></td>
      <td><script type="math/tex">\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i</script> (*)</td>
      <td><script type="math/tex">P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})</script></td>
    </tr>
  </tbody>
</table>

<p>(*) <script type="math/tex">k_\theta</script> 是一个衡量<script type="math/tex">\mathbf{x}_i</script>和<script type="math/tex">\mathbf{x}</script>相似度的kernel function。</p>

<p>接下来我们会回顾各种方法的经典模型。</p>

<h2 id="metric-based">Metric-Based</h2>

<p>Metric-based meta-learning的核心思想类似于最近邻算法(<a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-NN分类</a>、<a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means聚类</a>)和<a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">核密度估计</a>。该类方法在已知标签的集合上预测出来的概率，是support set中的样本标签的加权和。 权重由核函数（kernal function）<script type="math/tex">k_\theta</script>算得，该权重代表着两个数据样本之间的相似性。</p>

<script type="math/tex; mode=display">P_\theta(y \vert \mathbf{x}, S) = \sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i</script>

<p>因此，学到一个好的核函数对于metric-based meta-learning模型至关重要。<a href="https://en.wikipedia.org/wiki/Similarity_learning#Metric_learning">Metric learning</a>正是针对该问题提出的方法，它的目标就是学到一个不同样本之间的metric或者说是距离函数。任务不同，好的metric的定义也不同。但它一定在任务空间上表示了输入之间的联系，并且能够帮助我们解决问题。</p>

<p>下面列出的所有方法都显式的学习了输入数据的嵌入向量（embedding vectors），并根据其设计合适的kernel function。</p>

<h3 id="convolutional-siamese-neural-network">Convolutional Siamese Neural Network</h3>

<p><a href="https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf">Siamese Neural Network</a>最早被提出用来解决笔迹验证问题，siamese network由两个孪生网络组成，这两个网络的输出被联合起来训练一个函数，用于学习一对数据输入之间的关系。这两个网络结构相同，共享参数，实际上就是一个网络在学习如何有效地embedding才能显现出一对数据之间的关系。顺便一提，这是LeCun 1994年的论文。</p>

<p><a href="http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf">Koch, Zemel &amp; Salakhutdinov (2015)</a>提出了一种用siamese网络做one-shot image classification的方法。首先，训练一个用于图片验证的siamese网络，分辨两张图片是否属于同一类。然后在测试时，siamese网络把测试输入和support set里面的所有图片进行比较，选择相似度最高的那张图片所属的类作为输出。</p>

<p><img src="/assets/images/2019-09-19-meta-learning/siamese-conv-net.png" alt="siamese" />
<em>Fig. 2. 卷积siamese网络用于few-shot image classification的例子。</em></p>

<ol>
  <li>首先，卷积siamese网络学习一个由多个卷积层组成的embedding函数<script type="math/tex">f_\theta</script>，把两张图片编码为特征向量。</li>
  <li>两个特征向量之间的L1距离可以表示为<script type="math/tex">\vert f_\theta(\mathbf{x}_i) - f_\theta(\mathbf{x}_j) \vert</script>。</li>
  <li>通过一个linear feedforward layer和sigmoid把距离转换为概率。这就是两张图片属于同一类的概率。</li>
  <li>loss函数就是cross entropy loss，因为label是二元的。</li>
</ol>

<!-- In this way, an efficient image embedding is trained so that the distance between two embeddings is proportional to the similarity between two images. -->

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p(\mathbf{x}_i, \mathbf{x}_j) &= \sigma(\mathbf{W}\vert f_\theta(\mathbf{x}_i) - f_\theta(\mathbf{x}_j) \vert) \\
\mathcal{L}(B) &= \sum_{(\mathbf{x}_i, \mathbf{x}_j, y_i, y_j)\in B} \mathbf{1}_{y_i=y_j}\log p(\mathbf{x}_i, \mathbf{x}_j) + (1-\mathbf{1}_{y_i=y_j})\log (1-p(\mathbf{x}_i, \mathbf{x}_j))
\end{aligned} %]]></script>

<p>Training batch <script type="math/tex">B</script>可以通过对图片做一些变形增加数据量。你也可以把L1距离替换成其他距离，比如L2距离、cosine距离等等。只要距离是可导的就可以。</p>

<p>给定一个支持集<script type="math/tex">S</script>和一个测试图片<script type="math/tex">\mathbf{x}</script>，最终预测的分类为：</p>

<script type="math/tex; mode=display">\hat{c}_S(\mathbf{x}) = c(\arg\max_{\mathbf{x}_i \in S} P(\mathbf{x}, \mathbf{x}_i))</script>

<p><script type="math/tex">c(\mathbf{x})</script>是图片<script type="math/tex">\mathbf{x}</script>的label，<script type="math/tex">\hat{c}(.)</script>是预测的label。</p>

<p>这里我们有一个假设：学到的embedding在未见过的分类上依然能很好的衡量图片间的距离。这个假设跟迁移学习中使用预训练模型所隐含的假设是一样的。比如，在ImageNet上预训练的模型，其学到的卷积特征表达方式对于其他图像任务也有帮助。但实际上当新任务与旧任务有所差别的时候，预训练模型的效果就没有那么好了。</p>

<h3 id="matching-networks">Matching Networks</h3>

<p><strong>Matching Networks</strong> (<a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">Vinyals et al., 2016</a>)的目标是：对于每一个给定的支持集<script type="math/tex">S=\{x_i, y_i\}_{i=1}^k</script> (<em>k-shot</em> classification)，分别学一个分类器<script type="math/tex">c_S</script>。 这个分类器给出了给定测试样本<script type="math/tex">\mathbf{x}</script>时，输出<script type="math/tex">y</script>的概率分布。这个分类器的输出被定义为支持集中一系列label的加权和，权重由一个注意力核（attention kernel）<script type="math/tex">a(\mathbf{x}, \mathbf{x}_i)</script>决定。权重应当与<script type="math/tex">\mathbf{x}</script>和<script type="math/tex">\mathbf{x}_i</script>间的相似度成正比。</p>

<p><img src="../assets/images/2019-09-19-meta-learning/matching-networks.png" width="70%" /></p>

<p><em>Fig. 3. Matching Networks结构。（图像来源: <a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">original paper</a>）</em></p>

<script type="math/tex; mode=display">c_S(\mathbf{x}) = P(y \vert \mathbf{x}, S) = \sum_{i=1}^k a(\mathbf{x}, \mathbf{x}_i) y_i
\text{, where }S=\{(\mathbf{x}_i, y_i)\}_{i=1}^k</script>

<p>Attention kernel由两个embedding function <script type="math/tex">f</script>和<script type="math/tex">g</script>决定。分别用于encoding测试样例和支持集样本。两个样本之间的注意力权重是经过softmax归一化后的，他们embedding vectors的cosine距离<script type="math/tex">\text{cosine}(.)</script>。</p>

<script type="math/tex; mode=display">a(\mathbf{x}, \mathbf{x}_i) = \frac{\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}_i))}{\sum_{j=1}^k\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}_j))}</script>

<h4 id="simple-embedding">Simple Embedding</h4>

<p>在简化版本里，embedding function是一个使用单样本作为输入的神经网络。而且我们可以假设<script type="math/tex">f=g</script>。</p>

<h4 id="full-context-embeddings">Full Context Embeddings</h4>

<p>Embeding vectors对于构建一个好的分类器至关重要。只把一个数据样本作为embedding function的输入，会导致很难高效的估计出整个特征空间。因此，Matching Network模型又进一步发展，通过把整个支持集<script type="math/tex">S</script>作为embedding function的额外输入来加强embedding的有效性，相当于给样本添加了语境，让embedding根据样本与支持集中样本的关系进行调整。</p>

<ul>
  <li>
    <p><script type="math/tex">g_\theta(\mathbf{x}_i, S)</script>在整个支持集<script type="math/tex">S</script>的语境下用一个双向LSTM来编码<script type="math/tex">\mathbf{x}_i</script>.</p>
  </li>
  <li>
    <p><script type="math/tex">f_\theta(\mathbf{x}, S)</script>在支持集<script type="math/tex">S</script>上使用read attention机制编码测试样本<script type="math/tex">\mathbf{x}</script>。</p>
    <ol>
      <li>首先测试样本经过一个简单的神经网络，比如CNN，以抽取基本特征<script type="math/tex">f'(\mathbf{x})</script>。</li>
      <li>然后，一个带有read attention vector的LSTM被训练用于生成部分hidden state：<br />
  <script type="math/tex">% <![CDATA[
\begin{aligned}
  \hat{\mathbf{h}}_t, \mathbf{c}_t &= \text{LSTM}(f'(\mathbf{x}), [\mathbf{h}_{t-1}, \mathbf{r}_{t-1}], \mathbf{c}_{t-1}) \\
  \mathbf{h}_t &= \hat{\mathbf{h}}_t + f'(\mathbf{x}) \\
  \mathbf{r}_{t-1} &= \sum_{i=1}^k a(\mathbf{h}_{t-1}, g(\mathbf{x}_i)) g(\mathbf{x}_i) \\
  a(\mathbf{h}_{t-1}, g(\mathbf{x}_i)) &= \text{softmax}(\mathbf{h}_{t-1}^\top g(\mathbf{x}_i)) = \frac{\exp(\mathbf{h}_{t-1}^\top g(\mathbf{x}_i))}{\sum_{j=1}^k \exp(\mathbf{h}_{t-1}^\top g(\mathbf{x}_j))}
  \end{aligned} %]]></script></li>
      <li>最终，如果我们做k步的读取<script type="math/tex">f(\mathbf{x}, S)=\mathbf{h}_K</script>。</li>
    </ol>
  </li>
</ul>

<p>这类embedding方法被称作“全语境嵌入”（Full Contextual Embeddings）。有意思的是，这类方法对于困难的任务（few-shot classification on mini ImageNet）有所帮助，但对于简单的任务却没有提升（Omniglot）。</p>

<p>Matching Networks的训练过程与测试时的推理过程是一致的，详情请回顾之前的<a href="#像测试一样训练">章节</a>。值得一提的是，Matching Networks的论文强调了训练和测试的条件应当一致的原则。</p>

<script type="math/tex; mode=display">\theta^* = \arg\max_\theta \mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_\theta(y\vert\mathbf{x}, S^L)]]</script>

<h3 id="relation-network">Relation Network</h3>

<p><strong>Relation Network (RN)</strong> (<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">Sung et al., 2018</a>)与<a href="#convolutional-siamese-neural-network">siamese network</a>有所相似，但有以下几个不同点：</p>
<ol>
  <li>两个样本间的相似系数不是由特征空间的L1距离决定的，而是由一个CNN分类器<script type="math/tex">g_\phi</script>预测的。两个样本<script type="math/tex">\mathbf{x}_i</script>和<script type="math/tex">\mathbf{x}_j</script>间的相似系数为<script type="math/tex">r_{ij} = g_\phi([\mathbf{x}_i, \mathbf{x}_j])</script>，其中<script type="math/tex">[.,.]</script>代表着concatenation。</li>
  <li>目标优化函数是MSE损失，而不是cross-entropy，因为RN在预测时更倾向于把相似系数预测过程作为一个regression问题，而不是二分类问题，<script type="math/tex">\mathcal{L}(B) = \sum_{(\mathbf{x}_i, \mathbf{x}_j, y_i, y_j)\in B} (r_{ij} - \mathbf{1}_{y_i=y_j})^2</script></li>
</ol>

<p style="width: 100%;" class="center"><img src="/assets/images/relation-network.png" alt="relation-network" /></p>
<p><em>Fig. 4. Relation Network architecture for a 5-way 1-shot problem with one query example. (Image source: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">original paper</a>)</em></p>

<p>(Note: There is another <a href="https://deepmind.com/blog/neural-approach-relational-reasoning/">Relation Network</a> for relational reasoning, proposed by DeepMind. Don’t get confused.)</p>

<h3 id="prototypical-networks">Prototypical Networks</h3>

<p><strong>Prototypical Networks</strong> (<a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">Snell, Swersky &amp; Zemel, 2017</a>) use an embedding function <script type="math/tex">f_\theta</script> to encode each input into a <script type="math/tex">M</script>-dimensional feature vector. A <em>prototype</em> feature vector is defined for every class <script type="math/tex">c \in \mathcal{C}</script>, as the mean vector of the embedded support data samples in this class.</p>

<script type="math/tex; mode=display">\mathbf{v}_c = \frac{1}{|S_c|} \sum_{(\mathbf{x}_i, y_i) \in S_c} f_\theta(\mathbf{x}_i)</script>

<p style="width: 100%;" class="center"><img src="/assets/images/prototypical-networks.png" alt="prototypical-networks" /></p>
<p><em>Fig. 5. Prototypical networks in the few-shot and zero-shot scenarios. (Image source: <a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">original paper</a>)</em></p>

<p>The distribution over classes for a given test input <script type="math/tex">\mathbf{x}</script> is a softmax over the inverse of distances between the test data embedding and prototype vectors.</p>

<script type="math/tex; mode=display">P(y=c\vert\mathbf{x})=\text{softmax}(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_c)) = \frac{\exp(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_c))}{\sum_{c' \in \mathcal{C}}\exp(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_{c'}))}</script>

<p>where <script type="math/tex">d_\varphi</script> can be any distance function as long as <script type="math/tex">\varphi</script> is differentiable. In the paper, they used the squared euclidean distance.</p>

<p>The loss function is the negative log-likelihood: <script type="math/tex">\mathcal{L}(\theta) = -\log P_\theta(y=c\vert\mathbf{x})</script>.</p>

<h2 id="model-based">Model-Based</h2>

<p>Model-based meta-learning models make no assumption on the form of <script type="math/tex">P_\theta(y\vert\mathbf{x})</script>. Rather it depends on a model designed specifically for fast learning — a model that updates its parameters rapidly with a few training steps. This rapid parameter update can be achieved by its internal architecture or controlled by another meta-learner model.</p>

<h3 id="memory-augmented-neural-networks">Memory-Augmented Neural Networks</h3>

<p>A family of model architectures use external memory storage to facilitate the learning process of neural networks, including <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines">Neural Turing Machines</a> and <a href="https://arxiv.org/abs/1410.3916">Memory Networks</a>. With an explicit storage buffer, it is easier for the network to rapidly incorporate new information and not to forget in the future. Such a model is known as <strong>MANN</strong>, short for “<strong>Memory-Augmented Neural Network</strong>”.  Note that recurrent neural networks with only <em>internal memory</em> such as vanilla RNN or LSTM are not MANNs.</p>

<p>Because MANN is expected to encode new information fast and thus to adapt to new tasks after only a few samples, it fits well for meta-learning. Taking the Neural Turing Machine (NTM) as the base model, <a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et al. (2016)</a> proposed a set of modifications on the training setup and the memory retrieval mechanisms (or “addressing mechanisms”, deciding how to assign attention weights to memory vectors). Please go through <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines">the NTM section</a> in my other post first if you are not familiar with this matter before reading forward.</p>

<p>As a quick recap, NTM couples a controller neural network with external memory storage. The controller learns to read and write memory rows by soft attention, while the memory serves as a knowledge repository. The attention weights are generated by its addressing mechanism: content-based + location based.</p>

<p style="width: 70%;" class="center"><img src="/assets/images/NTM.png" alt="NTM" /></p>
<p><em>Fig. 6. The architecture of Neural Turing Machine (NTM). The memory at time t, <script type="math/tex">\mathbf{M}_t</script> is a matrix of size <script type="math/tex">N \times M</script>, containing N vector rows and each has M dimensions.</em></p>

<h4 id="mann-for-meta-learning">MANN for Meta-Learning</h4>

<p>To use MANN for meta-learning tasks, we need to train it in a way that the memory can encode and capture information of new tasks fast and, in the meantime, any stored representation is easily and stably accessible.</p>

<p>The training described in <a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et al., 2016</a> happens in an interesting way so that the memory is forced to hold information for longer until the appropriate labels are presented later. In each training episode, the truth label <script type="math/tex">y_t</script> is presented with <strong>one step offset</strong>, <script type="math/tex">(\mathbf{x}_{t+1}, y_t)</script>: it is the true label for the input at the previous time step t, but presented as part of the input at time step t+1.</p>

<p style="width: 100%;" class="center"><img src="/assets/images/mann-meta-learning.png" alt="NTM" /></p>
<p><em>Fig. 7. Task setup in MANN for meta-learning (Image source: <a href="http://proceedings.mlr.press/v48/santoro16.pdf">original paper</a>).</em></p>

<p>In this way, MANN is motivated to memorize the information of a new dataset, because the memory has to hold the current input until the label is present later and then retrieve the old information to make a prediction accordingly.</p>

<p>Next let us see how the memory is updated for efficient information retrieval and storage.</p>

<h4 id="addressing-mechanism-for-meta-learning">Addressing Mechanism for Meta-Learning</h4>

<p>Aside from the training process, a new pure content-based addressing mechanism is utilized to make the model better suitable for meta-learning.</p>

<p><strong>» How to read from memory?</strong>
<br />
The read attention is constructed purely based on the content similarity.</p>

<p>First, a key feature vector <script type="math/tex">\mathbf{k}_t</script> is produced at the time step t by the controller as a function of the input <script type="math/tex">\mathbf{x}</script>. Similar to NTM, a read weighting vector <script type="math/tex">\mathbf{w}_t^r</script> of N elements is computed as the cosine similarity between the key vector and every memory vector row, normalized by softmax. The read vector <script type="math/tex">\mathbf{r}_t</script> is a sum of memory records weighted by such weightings:</p>

<script type="math/tex; mode=display">\mathbf{r}_i = \sum_{i=1}^N w_t^r(i)\mathbf{M}_t(i)
\text{, where } w_t^r(i) = \text{softmax}(\frac{\mathbf{k}_t \cdot \mathbf{M}_t(i)}{\|\mathbf{k}_t\| \cdot \|\mathbf{M}_t(i)\|})</script>

<p>where <script type="math/tex">M_t</script> is the memory matrix at time t and <script type="math/tex">M_t(i)</script> is the i-th row in this matrix.</p>

<p><strong>» How to write into memory?</strong>
<br />
The addressing mechanism for writing newly received information into memory operates a lot like the <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies">cache replacement</a> policy. The <strong>Least Recently Used Access (LRUA)</strong> writer is designed for MANN to better work in the scenario of meta-learning. A LRUA write head prefers to write new content to either the <em>least used</em> memory location or the <em>most recently used</em> memory location.</p>
<ul>
  <li>Rarely used locations: so that we can preserve frequently used information (see <a href="https://en.wikipedia.org/wiki/Least_frequently_used">LFU</a>);</li>
  <li>The last used location: the motivation is that once a piece of information is retrieved once, it probably won’t be called again for a while (see <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Most_recently_used_(MRU)">MRU</a>).</li>
</ul>

<p>There are many cache replacement algorithms and each of them could potentially replace the design here with better performance in different use cases. Furthermore, it would be a good idea to learn the memory usage pattern and addressing strategies rather than arbitrarily set it.</p>

<p>The preference of LRUA is carried out in a way that everything is differentiable:</p>
<ol>
  <li>The usage weight <script type="math/tex">\mathbf{w}^u_t</script> at time t is a sum of current read and write vectors, in addition to the decayed last usage weight, <script type="math/tex">\gamma \mathbf{w}^u_{t-1}</script>, where <script type="math/tex">\gamma</script> is a decay factor.</li>
  <li>The write vector is an interpolation between the previous read weight (prefer “the last used location”) and the previous least-used weight (prefer “rarely used location”). The interpolation parameter is the sigmoid of a hyperparameter <script type="math/tex">\alpha</script>.</li>
  <li>The least-used weight <script type="math/tex">\mathbf{w}^{lu}</script> is scaled according to usage weights <script type="math/tex">\mathbf{w}_t^u</script>, in which any dimension remains at 1 if smaller than the n-th smallest element in the vector and 0 otherwise.</li>
</ol>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathbf{w}_t^u &= \gamma \mathbf{w}_{t-1}^u + \mathbf{w}_t^r + \mathbf{w}_t^w \\
\mathbf{w}_t^r &= \text{softmax}(\text{cosine}(\mathbf{k}_t, \mathbf{M}_t(i))) \\
\mathbf{w}_t^w &= \sigma(\alpha)\mathbf{w}_{t-1}^r + (1-\sigma(\alpha))\mathbf{w}^{lu}_{t-1}\\
\mathbf{w}_t^{lu} &= \mathbf{1}_{w_t^u(i) \leq m(\mathbf{w}_t^u, n)}
\text{, where }m(\mathbf{w}_t^u, n)\text{ is the }n\text{-th smallest element in vector }\mathbf{w}_t^u\text{.}
\end{aligned} %]]></script>

<p>Finally, after the least used memory location, indicated by <script type="math/tex">\mathbf{w}_t^{lu}</script>, is set to zero, every memory row is updated:</p>

<script type="math/tex; mode=display">\mathbf{M}_t(i) = \mathbf{M}_{t-1}(i) + w_t^w(i)\mathbf{k}_t, \forall i</script>

<h3 id="meta-networks">Meta Networks</h3>

<p><strong>Meta Networks</strong> (<a href="https://arxiv.org/abs/1703.00837">Munkhdalai &amp; Yu, 2017</a>), short for <strong>MetaNet</strong>, is a meta-learning model with architecture and training process designed for <em>rapid</em> generalization across tasks.</p>

<h4 id="fast-weights">Fast Weights</h4>

<p>The rapid generalization of MetaNet relies on “fast weights”. There are a handful of papers on this topic, but I haven’t read all of them in detail and I failed to find a very concrete definition, only a vague agreement on the concept. Normally weights in the neural networks are updated by stochastic gradient descent in an objective function and this process is known to be slow. One faster way to learn is to utilize one neural network to predict the parameters of another neural network and the generated weights are called <em>fast weights</em>. In comparison, the ordinary SGD-based weights are named <em>slow weights</em>.</p>

<p>In MetaNet, loss gradients are used as <em>meta information</em> to populate models that learn fast weights. Slow and fast weights are combined to make predictions in neural networks.</p>

<p style="width: 50%;" class="center"><img src="/assets/images/combine-slow-fast-weights.png" alt="slow-fast-weights" /></p>
<p><em>Fig. 8. Combining slow and fast weights in a MLP. <script type="math/tex">\bigoplus</script> is element-wise sum. (Image source: <a href="https://arxiv.org/abs/1703.00837">original paper</a>).</em></p>

<h4 id="model-components">Model Components</h4>

<blockquote>
  <p>Disclaimer: Below you will find my annotations are different from those in the paper. imo, the paper is poorly written, but the idea is still interesting. So I’m presenting the idea in my own language.</p>
</blockquote>

<p>Key components of MetaNet are:</p>
<ul>
  <li>An embedding function <script type="math/tex">f_\theta</script>, parameterized by <script type="math/tex">\theta</script>, encodes raw inputs into feature vectors. Similar to <a href="#convolutional-siamese-neural-network">Siamese Neural Network</a>, these embeddings are trained to be useful for telling whether two inputs are of the same class (verification task).</li>
  <li>A base learner model <script type="math/tex">g_\phi</script>, parameterized by weights <script type="math/tex">\phi</script>, completes the actual learning task.</li>
</ul>

<p>If we stop here, it looks just like <a href="#relation-network">Relation Network</a>. MetaNet, in addition, explicitly models the fast weights of both functions and then aggregates them back into the model (See Fig. 8).</p>

<p>Therefore we need additional two functions to output fast weights for <script type="math/tex">f</script> and <script type="math/tex">g</script> respectively.</p>
<ul>
  <li><script type="math/tex">F_w</script>: a LSTM parameterized by <script type="math/tex">w</script> for learning fast weights <script type="math/tex">\theta^+</script> of the embedding function <script type="math/tex">f</script>. It takes as input gradients of <script type="math/tex">f</script>’s embedding loss for verification task.</li>
  <li><script type="math/tex">G_v</script>: a neural network parameterized by <script type="math/tex">v</script> learning fast weights <script type="math/tex">\phi^+</script> for the base learner <script type="math/tex">g</script> from its loss gradients. In MetaNet, the learner’s loss gradients are viewed as the <em>meta information</em> of the task.</li>
</ul>

<p>Ok, now let’s see how meta networks are trained. The training data contains multiple pairs of datasets: a support set <script type="math/tex">S=\{\mathbf{x}'_i, y'_i\}_{i=1}^K</script> and a test set  <script type="math/tex">U=\{\mathbf{x}_i, y_i\}_{i=1}^L</script>. Recall that we have four networks and four sets of model parameters to learn, <script type="math/tex">(\theta, \phi, w, v)</script>.</p>

<p style="width: 90%;" class="center"><img src="/assets/images/meta-network.png" alt="meta-net" /></p>
<p><em>Fig.9. The MetaNet architecture.</em></p>

<h4 id="training-process">Training Process</h4>

<ol>
  <li>Sample a random pair of inputs at each time step t from the support set <script type="math/tex">S</script>, <script type="math/tex">(\mathbf{x}'_i, y'_i)</script> and <script type="math/tex">(\mathbf{x}'_j, y_j)</script>. Let <script type="math/tex">\mathbf{x}_{(t,1)}=\mathbf{x}'_i</script> and <script type="math/tex">\mathbf{x}_{(t,2)}=\mathbf{x}'_j</script>.<br />
for <script type="math/tex">t = 1, \dots, K</script>:
    <ul>
      <li>a. Compute a loss for representation learning; i.e., cross entropy for the verification task:<br />
<script type="math/tex">\mathcal{L}^\text{emb}_t = \mathbf{1}_{y'_i=y'_j} \log P_t + (1 - \mathbf{1}_{y'_i=y'_j})\log(1 - P_t)\text{, where }P_t = \sigma(\mathbf{W}\vert f_\theta(\mathbf{x}_{(t,1)}) - f_\theta(\mathbf{x}_{(t,2)})\vert)</script></li>
    </ul>
  </li>
  <li>Compute the task-level fast weights:
<script type="math/tex">\theta^+ = F_w(\nabla_\theta \mathcal{L}^\text{emb}_1, \dots, \mathcal{L}^\text{emb}_T)</script></li>
  <li>Next go through examples in the support set <script type="math/tex">S</script> and compute the example-level fast weights. Meanwhile, update the memory with learned representations.<br />
for <script type="math/tex">i=1, \dots, K</script>:
    <ul>
      <li>a. The base learner outputs a probability distribution: <script type="math/tex">P(\hat{y}_i \vert \mathbf{x}_i) = g_\phi(\mathbf{x}_i)</script> and the loss can be cross-entropy or MSE: <script type="math/tex">\mathcal{L}^\text{task}_i = y'_i \log g_\phi(\mathbf{x}'_i) + (1- y'_i) \log (1 - g_\phi(\mathbf{x}'_i))</script></li>
      <li>b. Extract meta information (loss gradients) of the task and compute the example-level fast weights:
<script type="math/tex">\phi_i^+ = G_v(\nabla_\phi\mathcal{L}^\text{task}_i)</script>
        <ul>
          <li>Then store <script type="math/tex">\phi^+_i</script> into <script type="math/tex">i</script>-th location of the “value” memory <script type="math/tex">\mathbf{M}</script>.<br /></li>
        </ul>
      </li>
      <li>d. Encode the support sample into a task-specific input representation using both slow and fast weights: <script type="math/tex">r'_i = f_{\theta, \theta^+}(\mathbf{x}'_i)</script>
        <ul>
          <li>Then store <script type="math/tex">r'_i</script> into <script type="math/tex">i</script>-th location of the “key” memory <script type="math/tex">\mathbf{R}</script>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Finally it is the time to construct the training loss using the test set <script type="math/tex">U=\{\mathbf{x}_i, y_i\}_{i=1}^L</script>.<br />
Starts with <script type="math/tex">\mathcal{L}_\text{train}=0</script>:<br />
for <script type="math/tex">j=1, \dots, L</script>:
    <ul>
      <li>a. Encode the test sample into a task-specific input representation:
<script type="math/tex">r_j = f_{\theta, \theta^+}(\mathbf{x}_j)</script></li>
      <li>b. The fast weights are computed by attending to representations of support set samples in memory <script type="math/tex">\mathbf{R}</script>. The attention function is of your choice. Here MetaNet uses cosine similarity:<br />
  <script type="math/tex">% <![CDATA[
\begin{aligned}
 a_j &= \text{cosine}(\mathbf{R}, r_j) = [\frac{r'_1\cdot r_j}{\|r'_1\|\cdot\|r_j\|}, \dots, \frac{r'_N\cdot r_j}{\|r'_N\|\cdot\|r_j\|}]\\
 \phi^+_j &= \text{softmax}(a_j)^\top \mathbf{M}
 \end{aligned} %]]></script></li>
      <li>c. Update the training loss: <script type="math/tex">\mathcal{L}_\text{train} \leftarrow \mathcal{L}_\text{train} + \mathcal{L}^\text{task}(g_{\phi, \phi^+}(\mathbf{x}_i), y_i)</script></li>
    </ul>
  </li>
  <li>Update all the parameters <script type="math/tex">(\theta, \phi, w, v)</script> using <script type="math/tex">\mathcal{L}_\text{train}</script>.</li>
</ol>

<h2 id="optimization-based">Optimization-Based</h2>

<p>Deep learning models learn through backpropagation of gradients. However, the gradient-based optimization is neither designed to cope with a small number of training samples, nor to converge within a small number of optimization steps. Is there a way to adjust the optimization algorithm so that the model can be good at learning with a few examples? This is what optimization-based approach meta-learning algorithms intend for.</p>

<h3 id="lstm-meta-learner">LSTM Meta-Learner</h3>

<p>The optimization algorithm can be explicitly modeled. <a href="https://openreview.net/pdf?id=rJY0-Kcll">Ravi &amp; Larochelle (2017)</a> did so and named it “meta-learner”, while the original model for handling the task is called “learner”. The goal of the meta-learner is to efficiently update the learner’s parameters using a small support set so that the learner can adapt to the new task quickly.</p>

<p>Let’s denote the learner model as <script type="math/tex">M_\theta</script> parameterized by <script type="math/tex">\theta</script>, the meta-learner as <script type="math/tex">R_\Theta</script> with parameters <script type="math/tex">\Theta</script>, and the loss function <script type="math/tex">\mathcal{L}</script>.</p>

<h4 id="why-lstm">Why LSTM?</h4>

<p>The meta-learner is modeled as a LSTM, because:</p>
<ol>
  <li>There is similarity between the gradient-based update in backpropagation and the cell-state update in LSTM.</li>
  <li>Knowing a history of gradients benefits the gradient update; think about how <a href="http://ruder.io/optimizing-gradient-descent/index.html#momentum">momentum</a> works.</li>
</ol>

<p>The update for the learner’s parameters at time step t with a learning rate <script type="math/tex">\alpha_t</script> is:</p>

<script type="math/tex; mode=display">\theta_t = \theta_{t-1} - \alpha_t \nabla_{\theta_{t-1}}\mathcal{L}_t</script>

<p>It has the same form as the cell state update in LSTM, if we set forget gate <script type="math/tex">f_t=1</script>, input gate <script type="math/tex">i_t = \alpha_t</script>, cell state <script type="math/tex">c_t = \theta_t</script>, and new cell state <script type="math/tex">\tilde{c}_t = -\nabla_{\theta_{t-1}}\mathcal{L}_t</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t\\
    &= \theta_{t-1} - \alpha_t\nabla_{\theta_{t-1}}\mathcal{L}_t
\end{aligned} %]]></script>

<p>While fixing <script type="math/tex">f_t=1</script> and <script type="math/tex">i_t=\alpha_t</script> might not be the optimal, both of them can be learnable and adaptable to different datasets.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
f_t &= \sigma(\mathbf{W}_f \cdot [\nabla_{\theta_{t-1}}\mathcal{L}_t, \mathcal{L}_t, \theta_{t-1}, f_{t-1}] + \mathbf{b}_f) & \scriptstyle{\text{; how much to forget the old value of parameters.}}\\
i_t &= \sigma(\mathbf{W}_i \cdot [\nabla_{\theta_{t-1}}\mathcal{L}_t, \mathcal{L}_t, \theta_{t-1}, i_{t-1}] + \mathbf{b}_i) & \scriptstyle{\text{; corresponding to the learning rate at time step t.}}\\
\tilde{\theta}_t &= -\nabla_{\theta_{t-1}}\mathcal{L}_t &\\
\theta_t &= f_t \odot \theta_{t-1} + i_t \odot \tilde{\theta}_t &\\
\end{aligned} %]]></script>

<h4 id="model-setup">Model Setup</h4>

<p style="width: 100%;" class="center"><img src="/assets/images/lstm-meta-learner.png" alt="lstm-meta-learner" /></p>
<p><em>Fig.10. How the learner <script type="math/tex">M_\theta</script> and the meta-learner <script type="math/tex">R_\Theta</script> are trained. (Image source: <a href="https://openreview.net/pdf?id=rJY0-Kcll">original paper</a> with more annotations)</em></p>

<p>The training process mimics what happens during test, since it has been proved to be beneficial in <a href="#matching-networks">Matching Networks</a>. During each training epoch, we first sample a dataset <script type="math/tex">\mathcal{D} = (\mathcal{D}_\text{train}, \mathcal{D}_\text{test}) \in \hat{\mathcal{D}}_\text{meta-train}</script> and then sample mini-batches out of <script type="math/tex">\mathcal{D}_\text{train}</script> to update <script type="math/tex">\theta</script> for <script type="math/tex">T</script> rounds. The final state of the learner parameter <script type="math/tex">\theta_T</script> is used to train the meta-learner on the test data <script type="math/tex">\mathcal{D}_\text{test}</script>.</p>

<p>Two implementation details to pay extra attention to:</p>
<ol>
  <li>How to compress the parameter space in LSTM meta-learner? As the meta-learner is modeling parameters of another neural network, it would have hundreds of thousands of variables to learn. Following the <a href="https://arxiv.org/abs/1606.04474">idea</a> of sharing parameters across coordinates,</li>
  <li>To simplify the training process, the meta-learner assumes that the loss <script type="math/tex">\mathcal{L}_t</script> and the gradient <script type="math/tex">\nabla_{\theta_{t-1}} \mathcal{L}_t</script> are independent.</li>
</ol>

<p style="width: 100%;" class="center"><img src="/assets/images/train-meta-learner.png" alt="train-meta-learner" /></p>

<h3 id="maml">MAML</h3>

<p><strong>MAML</strong>, short for <strong>Model-Agnostic Meta-Learning</strong> (<a href="https://arxiv.org/abs/1703.03400">Finn, et al. 2017</a>) is a fairly general optimization algorithm, compatible with any model that learns through gradient descent.</p>

<p>Let’s say our model is <script type="math/tex">f_\theta</script> with parameters <script type="math/tex">\theta</script>. Given a task <script type="math/tex">\tau_i</script> and its associated dataset <script type="math/tex">(\mathcal{D}^{(i)}_\text{train}, \mathcal{D}^{(i)}_\text{test})</script>, we can update the model parameters by one or more gradient descent steps (the following example only contains one step):</p>

<script type="math/tex; mode=display">\theta'_i = \theta - \alpha \nabla_\theta\mathcal{L}^{(0)}_{\tau_i}(f_\theta)</script>

<p>where <script type="math/tex">\mathcal{L}^{(0)}</script> is the loss computed using the mini data batch with id (0).</p>

<p style="width: 45%;" class="center"><img src="/assets/images/maml.png" alt="MAML" /></p>
<p><em>Fig. 11. Diagram of MAML. (Image source: <a href="https://arxiv.org/abs/1703.03400">original paper</a>)</em></p>

<p>Well, the above formula only optimizes for one task. To achieve a good generalization across a variety of tasks, we would like to find the optimal <script type="math/tex">\theta^*</script> so that the task-specific fine-tuning is more efficient. Now, we sample a new data batch with id (1) for updating the meta-objective. The loss, denoted as <script type="math/tex">\mathcal{L}^{(1)}</script>, depends on the mini batch (1). The superscripts in <script type="math/tex">\mathcal{L}^{(0)}</script> and <script type="math/tex">\mathcal{L}^{(1)}</script> only indicate different data batches, and they refer to the same loss objective for the same task.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta^* 
&= \arg\min_\theta \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta'_i}) = \arg\min_\theta \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta - \alpha\nabla_\theta \mathcal{L}_{\tau_i}^{(0)}(f_\theta)}) & \\
\theta &\leftarrow \theta - \beta \nabla_{\theta} \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta - \alpha\nabla_\theta \mathcal{L}_{\tau_i}^{(0)}(f_\theta)}) & \scriptstyle{\text{; updating rule}}
\end{aligned} %]]></script>

<p style="width: 60%;" class="center"><img src="/assets/images/maml-algo.png" alt="MAML Algorithm" /></p>
<p><em>Fig. 12. The general form of MAML algorithm. (Image source: <a href="https://arxiv.org/abs/1703.03400">original paper</a>)</em></p>

<h4 id="first-order-maml">First-Order MAML</h4>

<p>The meta-optimization step above relies on second derivatives. To make the computation less expensive, a modified version of MAML omits second derivatives, resulting in a simplified and cheaper implementation, known as <strong>First-Order MAML (FOMAML)</strong>.</p>

<p>Let’s consider the case of performing <script type="math/tex">k</script> inner gradient steps, <script type="math/tex">k\geq1</script>. Starting with the initial model parameter <script type="math/tex">\theta_\text{meta}</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta_0 &= \theta_\text{meta}\\
\theta_1 &= \theta_0 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_0)\\
\theta_2 &= \theta_1 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_1)\\
&\dots\\
\theta_k &= \theta_{k-1} - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_{k-1})
\end{aligned} %]]></script>

<p>Then in the outer loop, we sample a new data batch for updating the meta-objective.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta_\text{meta} &\leftarrow \theta_\text{meta} - \beta g_\text{MAML} & \scriptstyle{\text{; update for meta-objective}} \\[2mm]
\text{where } g_\text{MAML}
&= \nabla_{\theta} \mathcal{L}^{(1)}(\theta_k) &\\[2mm]
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot (\nabla_{\theta_{k-1}} \theta_k) \dots (\nabla_{\theta_0} \theta_1) \cdot (\nabla_{\theta} \theta_0) & \scriptstyle{\text{; following the chain rule}} \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k \nabla_{\theta_{i-1}} \theta_i &  \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k \nabla_{\theta_{i-1}} (\theta_{i-1} - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1})) &  \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k (I - \alpha\nabla_{\theta_{i-1}}(\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1}))) &
\end{aligned} %]]></script>

<p>The MAML gradient is:</p>

<script type="math/tex; mode=display">g_\text{MAML} = \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k (I - \alpha \color{red}{\nabla_{\theta_{i-1}}(\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1}))})</script>

<p>The First-Order MAML ignores the second derivative part in red. It is simplified as follows, equivalent to the derivative of the last inner gradient update result.</p>

<script type="math/tex; mode=display">g_\text{FOMAML} = \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k)</script>

<h3 id="reptile">Reptile</h3>

<p><strong>Reptile</strong> (<a href="https://arxiv.org/abs/1803.02999">Nichol, Achiam &amp; Schulman, 2018</a>) is a remarkably simple meta-learning optimization algorithm. It is similar to MAML in many ways, given that both rely on meta-optimization through gradient descent and both are model-agnostic.</p>

<p>The Reptile works by repeatedly:</p>
<ul>
  <li>1) sampling a task,</li>
  <li>2) training on it by multiple gradient descent steps,</li>
  <li>3) and then moving the model weights towards the new parameters.</li>
</ul>

<p>See the algorithm below:
<script type="math/tex">\text{SGD}(\mathcal{L}_{\tau_i}, \theta, k)</script> performs stochastic gradient update for k steps on the loss <script type="math/tex">\mathcal{L}_{\tau_i}</script> starting with initial parameter <script type="math/tex">\theta</script> and returns the final parameter vector. The batch version samples multiple tasks instead of one within each iteration. The reptile gradient is defined as <script type="math/tex">(\theta - W)/\alpha</script>, where <script type="math/tex">\alpha</script> is the stepsize used by the SGD operation.</p>

<p style="width: 52%;" class="center"><img src="/assets/images/reptile-algo.png" alt="Reptile Algorithm" /></p>
<p><em>Fig. 13. The batched version of Reptile algorithm. (Image source: <a href="https://arxiv.org/abs/1803.02999">original paper</a>)</em></p>

<p>At a glance, the algorithm looks a lot like an ordinary SGD. However, because the task-specific optimization can take more than one step. it eventually makes <script type="math/tex">\text{SGD}(\mathbb{E}
_\tau[\mathcal{L}_{\tau}], \theta, k)</script> diverge from <script type="math/tex">\mathbb{E}_\tau [\text{SGD}(\mathcal{L}_{\tau}, \theta, k)]</script> when k &gt; 1.</p>

<h4 id="the-optimization-assumption">The Optimization Assumption</h4>

<p>Assuming that a task <script type="math/tex">\tau \sim p(\tau)</script> has a manifold of optimal network configuration, <script type="math/tex">\mathcal{W}_{\tau}^*</script>. The model <script type="math/tex">f_\theta</script> achieves the best performance for task <script type="math/tex">\tau</script> when <script type="math/tex">\theta</script> lays on the surface of <script type="math/tex">\mathcal{W}_{\tau}^*</script>. To find a solution that is good across tasks, we would like to find a parameter close to all the optimal manifolds of all tasks:</p>

<script type="math/tex; mode=display">\theta^* = \arg\min_\theta \mathbb{E}_{\tau \sim p(\tau)} [\frac{1}{2} \text{dist}(\theta, \mathcal{W}_\tau^*)^2]</script>

<p style="width: 50%;" class="center"><img src="/assets/images/reptile-optim.png" alt="Reptile Algorithm" /></p>
<p><em>Fig. 14. The Reptile algorithm updates the parameter alternatively to be closer to the optimal manifolds of different tasks. (Image source: <a href="https://arxiv.org/abs/1803.02999">original paper</a>)</em></p>

<p>Let’s use the L2 distance as <script type="math/tex">\text{dist}(.)</script> and the distance between a point <script type="math/tex">\theta</script> and a set <script type="math/tex">\mathcal{W}_\tau^*</script> equals to the distance between <script type="math/tex">\theta</script> and a point <script type="math/tex">W_{\tau}^*(\theta)</script> on the manifold that is closest to <script type="math/tex">\theta</script>:</p>

<script type="math/tex; mode=display">\text{dist}(\theta, \mathcal{W}_{\tau}^*) = \text{dist}(\theta, W_{\tau}^*(\theta)) \text{, where }W_{\tau}^*(\theta) = \arg\min_{W\in\mathcal{W}_{\tau}^*} \text{dist}(\theta, W)</script>

<p>The gradient of the squared euclidean distance is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\nabla_\theta[\frac{1}{2}\text{dist}(\theta, \mathcal{W}_{\tau_i}^*)^2]
&= \nabla_\theta[\frac{1}{2}\text{dist}(\theta, W_{\tau_i}^*(\theta))^2] & \\
&= \nabla_\theta[\frac{1}{2}(\theta - W_{\tau_i}^*(\theta))^2] & \\
&= \theta - W_{\tau_i}^*(\theta) & \scriptstyle{\text{; See notes.}}
\end{aligned} %]]></script>

<p>Notes: According to the Reptile paper, “<em>the gradient of the squared euclidean distance between a point Θ and a set S is the vector 2(Θ − p), where p is the closest point in S to Θ</em>”. Technically the closest point in S is also a function of Θ, but I’m not sure why the gradient does not need to worry about the derivative of p. (Please feel free to leave me a comment or send me an email about this if you have ideas.)</p>

<p>Thus the update rule for one stochastic gradient step is:</p>

<script type="math/tex; mode=display">\theta = \theta - \alpha \nabla_\theta[\frac{1}{2} \text{dist}(\theta, \mathcal{W}_{\tau_i}^*)^2] = \theta - \alpha(\theta - W_{\tau_i}^*(\theta)) = (1-\alpha)\theta + \alpha W_{\tau_i}^*(\theta)</script>

<p>The closest point on the optimal task manifold <script type="math/tex">W_{\tau_i}^*(\theta)</script> cannot be computed exactly, but Reptile approximates it using <script type="math/tex">\text{SGD}(\mathcal{L}_\tau, \theta, k)</script>.</p>

<h4 id="reptile-vs-fomaml">Reptile vs FOMAML</h4>

<p>To demonstrate the deeper connection between Reptile and MAML, let’s expand the update formula with an example performing two gradient steps, k=2 in <script type="math/tex">\text{SGD}(.)</script>. Same as defined <a href="#maml">above</a>, <script type="math/tex">\mathcal{L}^{(0)}</script> and <script type="math/tex">\mathcal{L}^{(1)}</script> are losses using different mini-batches of data. For ease of reading, we adopt two simplified annotations: <script type="math/tex">g^{(i)}_j = \nabla_{\theta} \mathcal{L}^{(i)}(\theta_j)</script> and <script type="math/tex">H^{(i)}_j = \nabla^2_{\theta} \mathcal{L}^{(i)}(\theta_j)</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta_0 &= \theta_\text{meta}\\
\theta_1 &= \theta_0 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_0)= \theta_0 - \alpha g^{(0)}_0 \\
\theta_2 &= \theta_1 - \alpha\nabla_\theta\mathcal{L}^{(1)}(\theta_1) = \theta_0 - \alpha g^{(0)}_0 - \alpha g^{(1)}_1
\end{aligned} %]]></script>

<p>According to the <a href="#first-order-maml">early section</a>, the gradient of FOMAML is the last inner gradient update result. Therefore, when k=1:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= \nabla_{\theta_1} \mathcal{L}^{(1)}(\theta_1) = g^{(1)}_1 \\
g_\text{MAML} &= \nabla_{\theta_1} \mathcal{L}^{(1)}(\theta_1) \cdot (I - \alpha\nabla^2_{\theta} \mathcal{L}^{(0)}(\theta_0)) = g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1
\end{aligned} %]]></script>

<p>The Reptile gradient is defined as:</p>

<script type="math/tex; mode=display">g_\text{Reptile} = (\theta_0 - \theta_2) / \alpha = g^{(0)}_0 + g^{(1)}_1</script>

<p>Up to now we have:</p>

<p style="width: 50%;" class="center"><img src="/assets/images/reptile_vs_FOMAML.png" alt="Reptile vs FOMAML" /></p>
<p><em>Fig. 15. Reptile versus FOMAML in one loop of meta-optimization. (Image source: <a href="https://www.slideshare.net/YoonhoLee4/on-firstorder-metalearning-algorithms">slides</a> on Reptile by Yoonho Lee.)</em></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= g^{(1)}_1 \\
g_\text{MAML} &= g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1 \\
g_\text{Reptile} &= g^{(0)}_0 + g^{(1)}_1
\end{aligned} %]]></script>

<p>Next let’s try further expand <script type="math/tex">g^{(1)}_1</script> using <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansion</a>. Recall that Taylor expansion of a function <script type="math/tex">f(x)</script> that is differentiable at a number <script type="math/tex">a</script> is:</p>

<script type="math/tex; mode=display">f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \dots = \sum_{i=0}^\infty \frac{f^{(i)}(a)}{i!}(x-a)^i</script>

<p>We can consider <script type="math/tex">\nabla_{\theta}\mathcal{L}^{(1)}(.)</script> as a function and <script type="math/tex">\theta_0</script> as a value point. The Taylor expansion of <script type="math/tex">g_1^{(1)}</script> at the value point <script type="math/tex">\theta_0</script> is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_1^{(1)} &= \nabla_{\theta}\mathcal{L}^{(1)}(\theta_1) \\
&= \nabla_{\theta}\mathcal{L}^{(1)}(\theta_0) + \nabla^2_\theta\mathcal{L}^{(1)}(\theta_0)(\theta_1 - \theta_0) + \frac{1}{2}\nabla^3_\theta\mathcal{L}^{(1)}(\theta_0)(\theta_1 - \theta_0)^2 + \dots & \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + \frac{\alpha^2}{2}\nabla^3_\theta\mathcal{L}^{(1)}(\theta_0) (g_0^{(0)})^2 + \dots & \scriptstyle{\text{; because }\theta_1-\theta_0=-\alpha g_0^{(0)}} \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>Plug in the expanded form of <script type="math/tex">g_1^{(1)}</script> into the MAML gradients with one step inner gradient update:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= g^{(1)}_1 = g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
g_\text{MAML} &= g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1 \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2) - \alpha H^{(0)}_0 (g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2))\\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + \alpha^2 \alpha H^{(0)}_0 H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>The Reptile gradient becomes:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{Reptile} 
&= g^{(0)}_0 + g^{(1)}_1 \\
&= g^{(0)}_0 + g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>So far we have the formula of three types of gradients:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
g_\text{MAML} &= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + O(\alpha^2)\\
g_\text{Reptile}  &= g^{(0)}_0 + g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>During training, we often average over multiple data batches. In our example, the mini batches (0) and (1) are interchangeable since both are drawn at random. The expectation <script type="math/tex">\mathbb{E}_{\tau,0,1}</script> is averaged over two data batches, ids (0) and (1), for task <script type="math/tex">\tau</script>.</p>

<p>Let,</p>
<ul>
  <li><script type="math/tex">A = \mathbb{E}_{\tau,0,1} [g_0^{(0)}] = \mathbb{E}_{\tau,0,1} [g_0^{(1)}]</script>; it is the average gradient of task loss. We expect to improve the model parameter to achieve better task performance by following this direction pointed by <script type="math/tex">A</script>.</li>
  <li><script type="math/tex">B = \mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)} + H^{(0)}_0 g_0^{(1)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [\nabla_\theta(g^{(0)}_0 g_0^{(1)})]</script>; it is the direction (gradient) that increases the inner product of gradients of two different mini batches for the same task. We expect to improve the model parameter to achieve better generalization over different data by following this direction pointed by <script type="math/tex">B</script>.</li>
</ul>

<p>To conclude, both MAML and Reptile aim to optimize for the same goal, better task performance (guided by A) and better generalization (guided by B), when the gradient update is approximated by first three leading terms.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathbb{E}_{\tau,1,2}[g_\text{FOMAML}] &= A - \alpha B + O(\alpha^2)\\
\mathbb{E}_{\tau,1,2}[g_\text{MAML}] &= A - 2\alpha B + O(\alpha^2)\\
\mathbb{E}_{\tau,1,2}[g_\text{Reptile}]  &= 2A - \alpha B + O(\alpha^2)
\end{aligned} %]]></script>

<p>It is not clear to me whether the ignored term <script type="math/tex">O(\alpha^2)</script> might play a big impact on the parameter learning. But given that FOMAML is able to obtain a similar performance as the full version of MAML, it might be safe to say higher-level derivatives would not be critical during gradient descent update.</p>

<hr />

<p>Cited as:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{weng2018metalearning,
  title   = "Meta-Learning: Learning to Learn Fast",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "http://lilianweng.github.io/lil-log/2018/11/29/meta-learning.html"
}
</code></pre></div></div>

<p><em>If you notice mistakes and errors in this post, don’t hesitate to leave a comment or contact me at [lilian dot wengweng at gmail dot com] and I would be very happy to correct them asap.</em></p>

<p>See you in the next post!</p>

<h2 id="reference">Reference</h2>

<p>[1] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. <a href="https://www.cs.cmu.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf">“Human-level concept learning through probabilistic program induction.”</a> Science 350.6266 (2015): 1332-1338.</p>

<p>[2] Oriol Vinyals’ talk on <a href="http://metalearning-symposium.ml/files/vinyals.pdf">“Model vs Optimization Meta Learning”</a></p>

<p>[3] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. <a href="http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf">“Siamese neural networks for one-shot image recognition.”</a> ICML Deep Learning Workshop. 2015.</p>

<p>[4] Oriol Vinyals, et al. <a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">“Matching networks for one shot learning.”</a> NIPS. 2016.</p>

<p>[5] Flood Sung, et al. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">“Learning to compare: Relation network for few-shot learning.”</a> CVPR. 2018.</p>

<p>[6] Jake Snell, Kevin Swersky, and Richard Zemel. <a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">“Prototypical Networks for Few-shot Learning.”</a> CVPR. 2018.</p>

<p>[7] Adam Santoro, et al. <a href="http://proceedings.mlr.press/v48/santoro16.pdf">“Meta-learning with memory-augmented neural networks.”</a> ICML. 2016.</p>

<p>[8] Alex Graves, Greg Wayne, and Ivo Danihelka. <a href="https://arxiv.org/abs/1410.5401">“Neural turing machines.”</a> arXiv preprint arXiv:1410.5401 (2014).</p>

<p>[9] Tsendsuren Munkhdalai and Hong Yu. <a href="https://arxiv.org/abs/1703.00837">“Meta Networks.”</a> ICML. 2017.</p>

<p>[10] Sachin Ravi and Hugo Larochelle. <a href="https://openreview.net/pdf?id=rJY0-Kcll">“Optimization as a Model for Few-Shot Learning.”</a> ICLR. 2017.</p>

<p>[11] Chelsea Finn’s BAIR blog on <a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">“Learning to Learn”</a>.</p>

<p>[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. <a href="https://arxiv.org/abs/1703.03400">“Model-agnostic meta-learning for fast adaptation of deep networks.”</a> ICML 2017.</p>

<p>[13] Alex Nichol, Joshua Achiam, John Schulman. <a href="https://arxiv.org/abs/1803.02999">“On First-Order Meta-Learning Algorithms.”</a> arXiv preprint arXiv:1803.02999 (2018).</p>

<p>[14] <a href="https://www.slideshare.net/YoonhoLee4/on-firstorder-metalearning-algorithms">Slides on Reptile</a> by Yoonho Lee.</p>


  </div>


  <div class="page-navigation">
    
      <a class="prev" href="/%E7%9E%8E%E6%89%AF/2019/07/20/%E7%9E%8E%E6%89%AF.html">&larr; 瞎扯</a>
    

    
      <a class="next" href="/2019/09/23/gaussian-process.html">高斯过程回归科普 &rarr;</a>
    
  </div>

  
    <div id="disqus_thread"></div>
<!-- <script>
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://lilianweng-github-io-lil-log.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script> -->

<noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>

  

</article>

      </div>
    </main>

    <div style="clear: both;"/>
<footer class="site-footer">
    2018 &copy; Built by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> and <a href="https://github.com/jekyll/minima/" target="_blank">minima</a> | View <a href="https://github.com/Wei-TianHao/Wei-TianHao.github.io" target="_blank">this</a> on Github

    <p>
        <a href="//feed.xml" target="_blank">
            <img src="/assets/images/logo_rss.png" />
        </a>
        <!-- <a href="https://scholar.google.com/citations?user=dCa-pW8AAAAJ&hl=en&oi=ao" target="_blank">
            <img src="/assets/images/logo_scholar.png" />
        </a> -->
        <a href="https://github.com/Wei-TianHao" target="_blank">
            <img src="/assets/images/logo_github.png" />
        </a>
        <!-- <a href="https://www.instagram.com/lilianweng/" target="_blank">
            <img src="/assets/images/logo_instagram.png" />
        </a> -->
    </p>
</footer>


  </body>

</html>
