<!DOCTYPE html>
<html lang="en">

  <head>
    
      






    

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->

    <title>元学习: 学习如何学习【译】</title>

    <meta name="description" content="学习如何学习的方法被称为元学习。元学习的目标是在接触到没见过的任务或者迁移到新环境中时，可以根据之前的经验和少量的样本快速学习如何应对。元学习有三种常见的实现方法：1）学习有效的距离度量方式（基于度量的方法）；2）使用带有显式或隐式记忆储存的（循环）神经网络（基于模型的方法）；3）训练以快速学习为目标的模型（基于...">

    <meta content="TH的自言自语" property="og:site_name">
    
        <meta content="元学习: 学习如何学习【译】" property="og:title">
    
    
        <meta content="article" property="og:type">
    
    
        <meta content="学习如何学习的方法被称为元学习。元学习的目标是在接触到没见过的任务或者迁移到新环境中时，可以根据之前的经验和少量的样本快速学习如何应对。元学习有三种常见的实现方法：1）学习有效的距离度量方式（基于度量的方法）；2）使用带有显式或隐式记忆储存的（循环）神经网络（基于模型的方法）；3）训练以快速学习为目标的模型（基于优化的方法）" property="og:description">
    
    
        <meta content="http://localhost:4000/2019/09/16/meta-learning.html" property="og:url">
    
    
        <meta content="2019-09-16T20:00:00-04:00" property="article:published_time">
        <meta content="http://localhost:4000/about/" property="article:author">
    
    
    
        
        <meta content="meta-learning" property="article:tag">
        
        <meta content="long-read" property="article:tag">
        
    

    <link rel="shortcut icon" href="/blog/assets/images/favicon.png">
    <link rel="stylesheet" href="/blog/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/blog/2019/09/16/meta-learning.html">

    <!-- For Latex -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <!-- Google Analytics -->
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-8161570-6', 'auto');
        ga('send', 'pageview');
    </script>

    <!-- For Facebook share button -->
    <div id="fb-root"></div>
    <script>
      (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));
    </script>

    <!-- Twitter cards -->
    <meta name="twitter:site"    content="@wei_tianhao">
    <meta name="twitter:creator" content="@Tianhao Wei">
    <meta name="twitter:title"   content="元学习: 学习如何学习【译】">

    
        <meta name="twitter:description" content="<blockquote>
  <p>学习如何学习的方法被称为元学习。元学习的目标是在接触到没见过的任务或者迁移到新环境中时，可以根据之前的经验和少量的样本快速学习如何应对。元学习有三种常见的实现方法：1）学习有效的距离度量方式（基于度量的方法）；2）使用带有显式或隐式记忆储存的（循环）神经网络（基于模型的方法）；3）训练以快速学习为目标的模型（基于优化的方法）</p>
</blockquote>

">
    

    
        <meta name="twitter:card"  content="summary">
        <meta name="twitter:image" content="">
    
    <!-- end of Twitter cards -->

</head>


  <body>

    <header class="site-header" role="banner" id='header-bar'>

    <div class="wrapper">
        
        <a class="site-title" href="/blog/">TH的自言自语</a>

        <!-- <nav class="site-nav">
            <a class="page-link" href="https://wei-tianhao.github.io" target="_blank">&#x1f349; About</a>
        </nav> -->
        <!-- <nav class="site-nav">
            <a class="page-link" href="/blog/contact.html">&#x1f917; Contact</a>
        </nav>
        <nav class="site-nav">
            <a class="page-link" href="/blog/FAQ.html">&#x1F64B; FAQ</a>
        </nav>
        <nav class="site-nav">
            <a class="page-link" href="/blog/archive.html">&#x1f516; Archive</a>
        </nav> -->

        <nav class="site-nav">
            <a class="page-link" href="/blog/contact.html">&#x57;关于我</a>
        </nav>
        <nav class="site-nav">
            <a class="page-link" href="/blog/FAQ.html">FAQ</a>
        </nav>
        <nav class="site-nav">
            <a class="page-link" href="/blog/archive.html">归档</a>
        </nav>

    </div>

</header>

<script>
var prevScrollpos = window.pageYOffset;
window.onscroll = function() {
var currentScrollPos = window.pageYOffset;
  if (prevScrollpos > currentScrollPos) {
    document.getElementById("header-bar").style.top = "0";
  } else {
    document.getElementById("header-bar").style.top = "-50px";
  }
  prevScrollpos = currentScrollPos;
}
</script>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">元学习: 学习如何学习【译】</h1>
    <p class="post-meta">

      <time datetime="2019-09-16T20:00:00-04:00" itemprop="datePublished">
        
        Sep 16, 2019
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Tianhao Wei</span>
      </span>

      <span>
        
          
          <a class="post-tag" href="/blog/tag/meta-learning"><nobr>meta-learning</nobr>&nbsp;</a>
        
          
          <a class="post-tag" href="/blog/tag/long-read"><nobr>long-read</nobr>&nbsp;</a>
        
      </span>
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/2019/09/16/meta-learning.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>学习如何学习的方法被称为元学习。元学习的目标是在接触到没见过的任务或者迁移到新环境中时，可以根据之前的经验和少量的样本快速学习如何应对。元学习有三种常见的实现方法：1）学习有效的距离度量方式（基于度量的方法）；2）使用带有显式或隐式记忆储存的（循环）神经网络（基于模型的方法）；3）训练以快速学习为目标的模型（基于优化的方法）</p>
</blockquote>

<!--more-->

<p><strong>本文翻译自<a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html">Lilian</a>的英文博客，Lilian的博客质量非常高，大家多多关注~</strong></p>

<p>好的机器学习模型经常需要大量的数据来进行训练，但人却恰恰相反。小孩子看过一两次喵喵和小鸟后就能分辨出他们的区别。会骑自行车的人很快就能学会骑摩托车，有时候甚至不用人教。那么有没有可能让机器学习模型也具有相似的性质呢？如何才能让模型仅仅用少量的数据就学会新的概念和技能呢？这就是<strong>元学习</strong>要解决的问题。</p>

<p>我们期望好的元学习模型能够具备强大的适应能力和泛化能力。在测试时，模型会先经过一个自适应环节（adaptation process），即根据少量样本学习任务。经过自适应后，模型即可完成新的任务。自适应本质上来说就是一个短暂的学习过程，这就是为什么元学习也被称作<a href="https://www.cs.cmu.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf">“学习”学习</a>.</p>

<p>元学习可以解决的任务可以是任意一类定义好的机器学习任务，像是监督学习，强化学习等。具体的元学习任务例子有：</p>
<ul>
  <li>在没有猫的训练集上训练出来一个图片分类器，这个分类器需要在看过少数几张猫的照片后分辨出测试集的照片中有没有猫。</li>
  <li>训练一个玩游戏的AI，这个AI需要快速学会如何玩一个从来没玩过的游戏。</li>
  <li>一个仅在平地上训练过的机器人，需要在山坡上完成给定的任务。</li>
</ul>

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#元学习问题定义" id="markdown-toc-元学习问题定义">元学习问题定义</a>    <ul>
      <li><a href="#a-simple-view" id="markdown-toc-a-simple-view">A Simple View</a></li>
      <li><a href="#像测试一样训练" id="markdown-toc-像测试一样训练">像测试一样训练</a></li>
      <li><a href="#学习器和元学习器" id="markdown-toc-学习器和元学习器">学习器和元学习器</a></li>
      <li><a href="#常见方法" id="markdown-toc-常见方法">常见方法</a></li>
    </ul>
  </li>
  <li><a href="#基于度量的方法" id="markdown-toc-基于度量的方法">基于度量的方法</a>    <ul>
      <li><a href="#convolutional-siamese-neural-network" id="markdown-toc-convolutional-siamese-neural-network">Convolutional Siamese Neural Network</a></li>
      <li><a href="#matching-networks" id="markdown-toc-matching-networks">Matching Networks</a>        <ul>
          <li><a href="#simple-embedding" id="markdown-toc-simple-embedding">Simple Embedding</a></li>
          <li><a href="#full-context-embeddings" id="markdown-toc-full-context-embeddings">Full Context Embeddings</a></li>
        </ul>
      </li>
      <li><a href="#relation-network" id="markdown-toc-relation-network">Relation Network</a></li>
      <li><a href="#prototypical-networks" id="markdown-toc-prototypical-networks">Prototypical Networks</a></li>
    </ul>
  </li>
  <li><a href="#基于模型的方法" id="markdown-toc-基于模型的方法">基于模型的方法</a>    <ul>
      <li><a href="#memory-augmented-neural-networks" id="markdown-toc-memory-augmented-neural-networks">Memory-Augmented Neural Networks</a>        <ul>
          <li><a href="#mann-for-meta-learning" id="markdown-toc-mann-for-meta-learning">MANN for Meta-Learning</a></li>
          <li><a href="#addressing-mechanism-for-meta-learning" id="markdown-toc-addressing-mechanism-for-meta-learning">Addressing Mechanism for Meta-Learning</a></li>
        </ul>
      </li>
      <li><a href="#meta-networks" id="markdown-toc-meta-networks">Meta Networks</a>        <ul>
          <li><a href="#fast-weights" id="markdown-toc-fast-weights">Fast Weights</a></li>
          <li><a href="#model-components" id="markdown-toc-model-components">Model Components</a></li>
          <li><a href="#训练过程" id="markdown-toc-训练过程">训练过程</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#optimization-based" id="markdown-toc-optimization-based">Optimization-Based</a>    <ul>
      <li><a href="#lstm-meta-learner" id="markdown-toc-lstm-meta-learner">LSTM Meta-Learner</a>        <ul>
          <li><a href="#why-lstm" id="markdown-toc-why-lstm">Why LSTM?</a></li>
          <li><a href="#model-setup" id="markdown-toc-model-setup">Model Setup</a></li>
        </ul>
      </li>
      <li><a href="#maml" id="markdown-toc-maml">MAML</a>        <ul>
          <li><a href="#first-order-maml" id="markdown-toc-first-order-maml">First-Order MAML</a></li>
        </ul>
      </li>
      <li><a href="#reptile" id="markdown-toc-reptile">Reptile</a>        <ul>
          <li><a href="#the-optimization-assumption" id="markdown-toc-the-optimization-assumption">The Optimization Assumption</a></li>
          <li><a href="#reptile-vs-fomaml" id="markdown-toc-reptile-vs-fomaml">Reptile vs FOMAML</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="元学习问题定义">元学习问题定义</h2>

<p>在本文中，我们主要关注监督学习中的元学习任务，比如图像分类。在之后的文章中我们会继续讲解更有意思的元强化学习。</p>

<h3 id="a-simple-view">A Simple View</h3>

<p>我们现在假设有一个任务的分布，我们从这个分布中采样了许多任务作为训练集。好的元学习模型在这个训练集上训练后，应当对这个空间里所有的任务都具有良好的表现，即使是从来没见过的任务。每个任务可以表示为一个数据集 <script type="math/tex">\mathcal{D}</script> ，数据集中包括特征向量 <script type="math/tex">x</script> 和标签 <script type="math/tex">y</script> ，分布表示为 <script type="math/tex">p(\mathcal{D})</script> 。那么最佳的元学习模型参数可以表示为：</p>

<script type="math/tex; mode=display">\theta^* = \arg\min_\theta \mathbb{E}_{\mathcal{D}\sim p(\mathcal{D})} [\mathcal{L}_\theta(\mathcal{D})]</script>

<p>上式的形式跟一般的学习任务非常像，只不过上式中的每个<em>数据集</em>是一个<em>数据样本</em>。</p>

<p><em>少样本学习（Few-shot classification）</em> 是元学习的在监督学习中的一个实例。数据集 <script type="math/tex">\mathcal{D}</script> 经常被划分为两部分，一个用于学习的支持集（support set） <script type="math/tex">S</script> ，和一个用于训练和测试的预测集（prediction set） <script type="math/tex">B</script> ，即 <script type="math/tex">\mathcal{D}=\langle S, B\rangle</script> 。<em>K-shot N-class</em>分类任务，即支持集中有N类数据，每类数据有K个带有标注的样本。</p>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/few-shot-classification.png" alt="few-shot-classification" /></p>
<p><em>Fig. 1. 4-shot 2-class 图像分类的例子。 (图像来自<a href="https://www.pinterest.com/">Pinterest</a>)</em></p>

<h3 id="像测试一样训练">像测试一样训练</h3>

<p>一个数据集 <script type="math/tex">\mathcal{D}</script> 包含许多对特征向量和标签，即 <script type="math/tex">\mathcal{D} = \{(\mathbf{x}_i, y_i)\}</script> 。每个标签属于一个标签类 <script type="math/tex">\mathcal{L}</script> 。假设我们的分类器 <script type="math/tex">f_\theta</script> 的输入是特征向量 <script type="math/tex">\mathbf{x}</script> ，输出是属于第 <script type="math/tex">y</script> 类的概率 <script type="math/tex">P_\theta(y\vert\mathbf{x})</script> ， <script type="math/tex">\theta</script> 是分类器的参数。</p>

<p>如果我们每次选一个 <script type="math/tex">B \subset \mathcal{D}</script> 作为训练的batch，则最佳的模型参数，应当能够最大化，多组batch的正确标签概率之和。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta^* &= {\arg\max}_{\theta} \mathbb{E}_{(\mathbf{x}, y)\in \mathcal{D}}[P_\theta(y \vert \mathbf{x})] &\\
\theta^* &= {\arg\max}_{\theta} \mathbb{E}_{B\subset \mathcal{D}}[\sum_{(\mathbf{x}, y)\in B}P_\theta(y \vert \mathbf{x})] & \scriptstyle{\text{; trained with mini-batches.}}
\end{aligned} %]]></script>

<p>few-shot classification的目标是，在小规模的support set上“快速学习”（类似fine-tuning）后，能够减少在prediction set上的预测误差。为了训练模型快速学习的能力，我们在训练的时候按照以下步骤：</p>
<ol>
  <li>采样一个标签的子集, <script type="math/tex">L\subset\mathcal{L}</script> .</li>
  <li>根据采样的标签子集，采样一个support set <script type="math/tex">S^L \subset \mathcal{D}</script> 和一个training batch <script type="math/tex">B^L \subset \mathcal{D}</script> 。 <script type="math/tex">S^L</script> 和 <script type="math/tex">B^L</script> 中的数据的标签都属于 <script type="math/tex">L</script> ，即 <script type="math/tex">y \in L, \forall (x, y) \in S^L, B^L</script> .</li>
  <li>把support set作为模型的输入，进行“快速学习”。注意，不同的算法具有不同的学习策略，但总的来说，这一步不会永久性更新模型参数。 <!-- , $$ \hat{y}=f_\theta(\mathbf{x}, S^L) $$ --></li>
  <li>把prediction set作为模型的输入，计算模型在 <script type="math/tex">B^L</script> 上的loss，根据这个loss进行反向传播更新模型参数。这一步与监督学习一致。</li>
</ol>

<p>你可以把每一对 <script type="math/tex">(S^L, B^L)</script> 看做是一个数据点。模型被训练出了在其他数据集上扩展的能力。下式中的红色部分是元学习的目标相比于监督学习的目标多出来的部分。</p>

<script type="math/tex; mode=display">\theta = \arg\max_\theta \color{red}{E_{L\subset\mathcal{L}}[} E_{\color{red}{S^L \subset\mathcal{D}, }B^L \subset\mathcal{D}} [\sum_{(x, y)\in B^L} P_\theta(x, y\color{red}{, S^L})] \color{red}{]}</script>

<p>这个想法有点像是我们面对某个只有少量数据的任务时，会使用在相关任务的大数据集上预训练的模型，然后进行fine-tuning。像是图形语义分割网络可以用在ImageNet上预训练的模型做初始化。相比于在一个特定任务上fine-tuning使得模型更好的拟合这个任务，元学习更进一步，它的目标是让模型优化以后能够在多个任务上表现的更好，类似于变得更容易被fine-tuning。</p>

<h3 id="学习器和元学习器">学习器和元学习器</h3>

<p>还有一种常见的看待meta-learning的视角，把模型的更新划分为了两个阶段：</p>
<ul>
  <li>根据给定的任务，训练一个分类器 <script type="math/tex">f_\theta</script> 完成任务，作为“学习器”模型</li>
  <li>同时，训练一个元学习器 <script type="math/tex">g_\phi</script> ，根据support set <script type="math/tex">S</script> 学习如何更新学习器模型的参数。 <script type="math/tex">\theta' = g_\phi(\theta, S)</script></li>
</ul>

<p>则最后的优化目标中，我们需要更新 <script type="math/tex">\theta</script> 和 <script type="math/tex">\phi</script> 来最大化：</p>

<script type="math/tex; mode=display">\mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]]</script>

<h3 id="常见方法">常见方法</h3>

<p>元学习主要有三类常见的方法：基于度量的方法（metric-based），基于模型的方法（model-based），基于优化的方法（optimization-based）。
Oriol Vinyals在NIPS 2018的meta-learning symposium上做了一个很好的<a href="http://metalearning-symposium.ml/files/vinyals.pdf">总结</a>：</p>

<table class="info">
  <thead>
    <tr>
      <th> </th>
      <th>Model-based</th>
      <th>Metric-based</th>
      <th>Optimization-based</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Key idea</strong></td>
      <td>RNN; memory</td>
      <td>Metric learning</td>
      <td>Gradient descent</td>
    </tr>
    <tr>
      <td><strong>How <script type="math/tex">P_\theta(y \vert \mathbf{x})</script> is modeled?</strong></td>
      <td><script type="math/tex">f_\theta(\mathbf{x}, S)</script></td>
      <td><script type="math/tex">\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i</script> (*)</td>
      <td><script type="math/tex">P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})</script></td>
    </tr>
  </tbody>
</table>

<p>(*) <script type="math/tex">k_\theta</script> 是一个衡量 <script type="math/tex">\mathbf{x}_i</script> 和 <script type="math/tex">\mathbf{x}</script> 相似度的kernel function。</p>

<p>接下来我们会回顾各种方法的经典模型。</p>

<h2 id="基于度量的方法">基于度量的方法</h2>

<p>基于度量的元学习的核心思想类似于最近邻算法(<a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-NN分类</a>、<a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means聚类</a>)和<a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">核密度估计</a>。该类方法在已知标签的集合上预测出来的概率，是support set中的样本标签的加权和。 权重由核函数（kernal function） <script type="math/tex">k_\theta</script> 算得，该权重代表着两个数据样本之间的相似性。</p>

<script type="math/tex; mode=display">P_\theta(y \vert \mathbf{x}, S) = \sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i</script>

<p>因此，学到一个好的核函数对于基于度量的元学习模型至关重要。<a href="https://en.wikipedia.org/wiki/Similarity_learning#Metric_learning">Metric learning</a>正是针对该问题提出的方法，它的目标就是学到一个不同样本之间的metric或者说是距离函数。任务不同，好的metric的定义也不同。但它一定在任务空间上表示了输入之间的联系，并且能够帮助我们解决问题。</p>

<p>下面列出的所有方法都显式的学习了输入数据的嵌入向量（embedding vectors），并根据其设计合适的kernel function。</p>

<h3 id="convolutional-siamese-neural-network">Convolutional Siamese Neural Network</h3>

<p><a href="https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf">Siamese Neural Network</a>最早被提出用来解决笔迹验证问题，siamese network由两个孪生网络组成，这两个网络的输出被联合起来训练一个函数，用于学习一对数据输入之间的关系。这两个网络结构相同，共享参数，实际上就是一个网络在学习如何有效地embedding才能显现出一对数据之间的关系。顺便一提，这是LeCun 1994年的论文。</p>

<p><a href="http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf">Koch, Zemel &amp; Salakhutdinov (2015)</a>提出了一种用siamese网络做one-shot image classification的方法。首先，训练一个用于图片验证的siamese网络，分辨两张图片是否属于同一类。然后在测试时，siamese网络把测试输入和support set里面的所有图片进行比较，选择相似度最高的那张图片所属的类作为输出。</p>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/siamese-conv-net.png" alt="siamese" /></p>
<p><em>Fig. 2. 卷积siamese网络用于few-shot image classification的例子。</em></p>

<ol>
  <li>首先，卷积siamese网络学习一个由多个卷积层组成的embedding函数 <script type="math/tex">f_\theta</script> ，把两张图片编码为特征向量。</li>
  <li>两个特征向量之间的L1距离可以表示为 <script type="math/tex">\vert f_\theta(\mathbf{x}_i) - f_\theta(\mathbf{x}_j) \vert</script> 。</li>
  <li>通过一个linear feedforward layer和sigmoid把距离转换为概率。这就是两张图片属于同一类的概率。</li>
  <li>loss函数就是cross entropy loss，因为label是二元的。</li>
</ol>

<!-- In this way, an efficient image embedding is trained so that the distance between two embeddings is proportional to the similarity between two images. -->

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p(\mathbf{x}_i, \mathbf{x}_j) &= \sigma(\mathbf{W}\vert f_\theta(\mathbf{x}_i) - f_\theta(\mathbf{x}_j) \vert) \\
\mathcal{L}(B) &= \sum_{(\mathbf{x}_i, \mathbf{x}_j, y_i, y_j)\in B} \mathbf{1}_{y_i=y_j}\log p(\mathbf{x}_i, \mathbf{x}_j) + (1-\mathbf{1}_{y_i=y_j})\log (1-p(\mathbf{x}_i, \mathbf{x}_j))
\end{aligned} %]]></script>

<p>Training batch <script type="math/tex">B</script> 可以通过对图片做一些变形增加数据量。你也可以把L1距离替换成其他距离，比如L2距离、cosine距离等等。只要距离是可导的就可以。</p>

<p>给定一个支持集 <script type="math/tex">S</script> 和一个测试图片 <script type="math/tex">\mathbf{x}</script> ，最终预测的分类为：</p>

<script type="math/tex; mode=display">\hat{c}_S(\mathbf{x}) = c(\arg\max_{\mathbf{x}_i \in S} P(\mathbf{x}, \mathbf{x}_i))</script>

<p><script type="math/tex">c(\mathbf{x})</script> 是图片 <script type="math/tex">\mathbf{x}</script> 的label， <script type="math/tex">\hat{c}(.)</script> 是预测的label。</p>

<p>这里我们有一个假设：学到的embedding在未见过的分类上依然能很好的衡量图片间的距离。这个假设跟迁移学习中使用预训练模型所隐含的假设是一样的。比如，在ImageNet上预训练的模型，其学到的卷积特征表达方式对于其他图像任务也有帮助。但实际上当新任务与旧任务有所差别的时候，预训练模型的效果就没有那么好了。</p>

<h3 id="matching-networks">Matching Networks</h3>

<p><strong>Matching Networks</strong> (<a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">Vinyals et al., 2016</a>)的目标是：对于每一个给定的支持集 <script type="math/tex">S=\{x_i, y_i\}_{i=1}^k</script> (<em>k-shot</em> classification)，分别学一个分类器 <script type="math/tex">c_S</script> 。 这个分类器给出了给定测试样本 <script type="math/tex">\mathbf{x}</script> 时，输出 <script type="math/tex">y</script> 的概率分布。这个分类器的输出被定义为支持集中一系列label的加权和，权重由一个注意力核（attention kernel） <script type="math/tex">a(\mathbf{x}, \mathbf{x}_i)</script> 决定。权重应当与 <script type="math/tex">\mathbf{x}</script> 和 <script type="math/tex">\mathbf{x}_i</script> 间的相似度成正比。</p>

<p style="width: 70%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/matching-networks.png" alt="siamese" /></p>
<p><em>Fig. 3. Matching Networks结构。（图像来源: <a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">原论文</a>）</em></p>

<script type="math/tex; mode=display">c_S(\mathbf{x}) = P(y \vert \mathbf{x}, S) = \sum_{i=1}^k a(\mathbf{x}, \mathbf{x}_i) y_i
\text{, where }S=\{(\mathbf{x}_i, y_i)\}_{i=1}^k</script>

<p>Attention kernel由两个embedding function <script type="math/tex">f</script> 和 <script type="math/tex">g</script> 决定。分别用于encoding测试样例和支持集样本。两个样本之间的注意力权重是经过softmax归一化后的，他们embedding vectors的cosine距离 <script type="math/tex">\text{cosine}(.)</script> 。</p>

<script type="math/tex; mode=display">a(\mathbf{x}, \mathbf{x}_i) = \frac{\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}_i))}{\sum_{j=1}^k\exp(\text{cosine}(f(\mathbf{x}), g(\mathbf{x}_j))}</script>

<h4 id="simple-embedding">Simple Embedding</h4>

<p>在简化版本里，embedding function是一个使用单样本作为输入的神经网络。而且我们可以假设 <script type="math/tex">f=g</script> 。</p>

<h4 id="full-context-embeddings">Full Context Embeddings</h4>

<p>Embeding vectors对于构建一个好的分类器至关重要。只把一个数据样本作为embedding function的输入，会导致很难高效的估计出整个特征空间。因此，Matching Network模型又进一步发展，通过把整个支持集 <script type="math/tex">S</script> 作为embedding function的额外输入来加强embedding的有效性，相当于给样本添加了语境，让embedding根据样本与支持集中样本的关系进行调整。</p>

<ul>
  <li>
    <p><script type="math/tex">g_\theta(\mathbf{x}_i, S)</script> 在整个支持集 <script type="math/tex">S</script> 的语境下用一个双向LSTM来编码 <script type="math/tex">\mathbf{x}_i</script> .</p>
  </li>
  <li>
    <p><script type="math/tex">f_\theta(\mathbf{x}, S)</script> 在支持集 <script type="math/tex">S</script> 上使用read attention机制编码测试样本 <script type="math/tex">\mathbf{x}</script> 。</p>
    <ol>
      <li>首先测试样本经过一个简单的神经网络，比如CNN，以抽取基本特征 <script type="math/tex">f'(\mathbf{x})</script> 。</li>
      <li>然后，一个带有read attention vector的LSTM被训练用于生成部分hidden state：<br />
  <script type="math/tex">% <![CDATA[
\begin{aligned}
  \hat{\mathbf{h}}_t, \mathbf{c}_t &= \text{LSTM}(f'(\mathbf{x}), [\mathbf{h}_{t-1}, \mathbf{r}_{t-1}], \mathbf{c}_{t-1}) \\
  \mathbf{h}_t &= \hat{\mathbf{h}}_t + f'(\mathbf{x}) \\
  \mathbf{r}_{t-1} &= \sum_{i=1}^k a(\mathbf{h}_{t-1}, g(\mathbf{x}_i)) g(\mathbf{x}_i) \\
  a(\mathbf{h}_{t-1}, g(\mathbf{x}_i)) &= \text{softmax}(\mathbf{h}_{t-1}^\top g(\mathbf{x}_i)) = \frac{\exp(\mathbf{h}_{t-1}^\top g(\mathbf{x}_i))}{\sum_{j=1}^k \exp(\mathbf{h}_{t-1}^\top g(\mathbf{x}_j))}
  \end{aligned} %]]></script></li>
      <li>最终，如果我们做k步的读取 <script type="math/tex">f(\mathbf{x}, S)=\mathbf{h}_K</script> 。</li>
    </ol>
  </li>
</ul>

<p>这类embedding方法被称作“全语境嵌入”（Full Contextual Embeddings）。有意思的是，这类方法对于困难的任务（few-shot classification on mini ImageNet）有所帮助，但对于简单的任务却没有提升（Omniglot）。</p>

<p>Matching Networks的训练过程与测试时的推理过程是一致的，详情请回顾之前的<a href="#像测试一样训练">章节</a>。值得一提的是，Matching Networks的论文强调了训练和测试的条件应当一致的原则。</p>

<script type="math/tex; mode=display">\theta^* = \arg\max_\theta \mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_\theta(y\vert\mathbf{x}, S^L)]]</script>

<h3 id="relation-network">Relation Network</h3>

<p><strong>Relation Network (RN)</strong> (<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">Sung et al., 2018</a>)与<a href="#convolutional-siamese-neural-network">siamese network</a>比较像，但有以下几个不同点：</p>
<ol>
  <li>两个样本间的相似系数不是由特征空间的L1距离决定的，而是由一个CNN分类器 <script type="math/tex">g_\phi</script> 预测的。两个样本 <script type="math/tex">\mathbf{x}_i</script> 和 <script type="math/tex">\mathbf{x}_j</script> 间的相似系数为 <script type="math/tex">r_{ij} = g_\phi([\mathbf{x}_i, \mathbf{x}_j])</script> ，其中 <script type="math/tex">[.,.]</script> 代表着concatenation。</li>
  <li>目标优化函数是MSE损失，而不是cross-entropy，因为RN在预测时更倾向于把相似系数预测过程作为一个regression问题，而不是二分类问题， <script type="math/tex">\mathcal{L}(B) = \sum_{(\mathbf{x}_i, \mathbf{x}_j, y_i, y_j)\in B} (r_{ij} - \mathbf{1}_{y_i=y_j})^2</script></li>
</ol>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/relation-network.png" alt="relation-network" /></p>
<p><em>Fig. 4. Relation Network的结构，图中是一个5分类1-shot的例子。(图片来源：<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">原论文</a>)</em></p>

<p>(注意：还有一个<a href="https://deepmind.com/blog/neural-approach-relational-reasoning/">Relation Network</a>是DeepMind提出来用于关系推理的，不要搞混了。)</p>

<h3 id="prototypical-networks">Prototypical Networks</h3>

<p><strong>Prototypical Networks</strong> (<a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">Snell, Swersky &amp; Zemel, 2017</a>)用一个嵌入函数 <script type="math/tex">f_\theta</script> 把每个输入编码为一个M维特征向量。然后对每一类 <script type="math/tex">c \in \mathcal{C}</script> ，取所有支持集样本的特征向量的平均值作为这个类的prototype特征。</p>

<script type="math/tex; mode=display">\mathbf{v}_c = \frac{1}{|S_c|} \sum_{(\mathbf{x}_i, y_i) \in S_c} f_\theta(\mathbf{x}_i)</script>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/prototypical-networks.png" alt="prototypical-networks" /></p>
<p><em>Fig. 5. 在少样本学习和无样本学习中的Prototypical networks。(图像来源：<a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">原论文</a>)</em></p>

<p>测试样本属于各类的概率分布由特征向量和prototype向量的距离取负后通过softmanx得到。</p>

<script type="math/tex; mode=display">P(y=c\vert\mathbf{x})=\text{softmax}(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_c)) = \frac{\exp(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_c))}{\sum_{c' \in \mathcal{C}}\exp(-d_\varphi(f_\theta(\mathbf{x}), \mathbf{v}_{c'}))}</script>

<p>其中 <script type="math/tex">d_\varphi</script> 可以是任意距离函数，只要 <script type="math/tex">\varphi</script> 可导即可。这篇文章中，他们使用了平方欧氏距离。</p>

<p>损失函数用的是负对数似然: <script type="math/tex">\mathcal{L}(\theta) = -\log P_\theta(y=c\vert\mathbf{x})</script> .</p>

<h2 id="基于模型的方法">基于模型的方法</h2>

<p>基于模型的元学习方法不对 <script type="math/tex">P_\theta(y\vert\mathbf{x})</script> 作出任何假设。 <script type="math/tex">P_\theta(y\vert\mathbf{x})</script> 是由一个专门用来快速学习的模型生成的，快速学习指的是这个模型可以根据少量的训练快速更新参数。有两种方式可以实现快速学习，1.设计好模型的内部架构使其能够快速学习，2.用另外一个模型来生成快速学习模型的参数。</p>

<h3 id="memory-augmented-neural-networks">Memory-Augmented Neural Networks</h3>

<p>许多模型架构使用了外部储存来帮助神经网络学习，像是<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines">Neural Turing Machines</a>和<a href="https://arxiv.org/abs/1410.3916">Memory Networks</a>。使用外部存储，让神经网络能够更容易的学到新知识并提供给以后使用。这样的模型被称为<strong>MANN</strong>（”<strong>Memory-Augmented Neural Network</strong>“）。注意，只使用了<em>内部存储</em>的循环神经网络并不是MANN，比如RNN、LSTM。</p>

<p>MANN的目的是在仅给定几个训练样本的情况下，快速编码新的信息并适应新的任务，因此MANN非常适合用于元学习。<a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et al. (2016)</a>以Neural Turing Machine（NTM）为基础，对训练和存储读写机制（也称为“寻址机制”，即如何对存储向量分配注意力权重）做了一系列的修改。如果你对NTM或寻址机制不太熟悉的话，可以参见原作者之前博文中的<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines">NTM介绍</a>。</p>

<p>简要回顾一下，NTM由一个控制器神经网络和存储器组成。控制器学习如何通过软注意力（soft attention）读写存储器，而存储器相当于是一个知识库。注意力权重是由寻址机制生成的，由询问的内容和位置共同决定。</p>

<p style="width: 70%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/NTM.png" alt="NTM" /></p>
<p><em>Fig. 6. NTM的架构，t时刻的存储， <script type="math/tex">\mathbf{M}_t</script> 是一个大小为 <script type="math/tex">N \times M</script> 的矩阵，代表着N个M维的向量，每个向量是一条记录</em></p>

<h4 id="mann-for-meta-learning">MANN for Meta-Learning</h4>

<p>为了在元学习中使用MANN，我们需要训练这个网络使得它能够快速提炼新任务的信息，而且能够快速稳定的查询之前存储的特征。</p>

<p><a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et al., 2016</a>提出了一种有意思的训练方式，他们强迫存储器保留当前样本的信息直到对应的标签出现。在每个episode中，标签有 <script type="math/tex">一步的延迟</script> ，即每次给出的训练对为 <script type="math/tex">(\mathbf{x}_{t+1}, y_t)</script> 。</p>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/mann-meta-learning.png" alt="NTM" /></p>
<p><em>Fig. 7. MANN在元学习中的任务设置（图像来源：<a href="http://proceedings.mlr.press/v48/santoro16.pdf">原论文</a>）。</em></p>

<p>通过这种设定，MANN会学到要记住新数据集的信息，因为存储器需要保留着当前输入的信息，并且在对应的标签出现的时候取回之前存储的信息进行预测。</p>

<p>接下来，我们看看存储器是完成信息的存储和取回的。</p>

<h4 id="addressing-mechanism-for-meta-learning">Addressing Mechanism for Meta-Learning</h4>

<p>为了让模型更加适应元学习，除了改变训练过程，作者还增加了一个完全基于内容的寻址机制。</p>

<p><strong>» 如何从存储器中取回信息?</strong>
<br /></p>

<p>读取注意力（read attention）完全由内容相似度决定。</p>

<p>首先，根据t时刻的输入 <script type="math/tex">\mathbf{x}</script> ，控制器生成一个键值特征向量 <script type="math/tex">\mathbf{k}_t</script> 。然后用类似于NTM的方法，计算键值特征向量和存储器中每个向量的cosine距离，经过softmax归一化，得到一个读取权重向量 <script type="math/tex">\mathbf{w}_t^r</script> 。读取向量 <script type="math/tex">\mathbf{r}_t</script> 是对存储器中所有向量的加权和：</p>

<script type="math/tex; mode=display">\mathbf{r}_i = \sum_{i=1}^N w_t^r(i)\mathbf{M}_t(i)
\text{, where } w_t^r(i) = \text{softmax}(\frac{\mathbf{k}_t \cdot \mathbf{M}_t(i)}{\|\mathbf{k}_t\| \cdot \|\mathbf{M}_t(i)\|})</script>

<p><script type="math/tex">M_t</script> 是t时刻的存储器矩阵， <script type="math/tex">M_t(i)</script> 是该矩阵中的第i行，即第i个向量。</p>

<p><strong>» 如何往存储器中写入信息?</strong>
<br /></p>

<p>写入新信息的寻址机制跟<a href="https://en.wikipedia.org/wiki/Cache_replacement_policies">缓存置换机制</a>很像。为了更好的适应元学习的任务，MANN使用的是<strong>最近最少使用算法(Least Recently Used Access, LRUA)</strong>。LRUA算法会优先覆盖<em>最少</em>使用的，或者<em>最近</em>刚用过的存储位置。</p>

<ul>
  <li>最少使用的位置：目的是保存经常使用的那些信息(参见<a href="https://en.wikipedia.org/wiki/Least_frequently_used">LFU</a>);</li>
  <li>最近使用的位置：原因是刚用过的信息很有可能不会马上用到(参见<a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Most_recently_used_(MRU)">MRU</a>).</li>
</ul>

<p>除了LRUA，还有许多其他的缓存置换算法。根据场景的不同，其他的算法可能有更好的表现。另外相比于人为指定一种缓存替换机制，根据存储使用的规律，学出一套寻址策略可能效果更好。</p>

<p>这里的LRUA以一种所有运算都可微分的方式实现：</p>
<ol>
  <li>t时刻的使用权重 <script type="math/tex">\mathbf{w}^u_t</script> 是当前读写向量的和，再加上上一时刻的使用权重 <script type="math/tex">\gamma \mathbf{w}^u_{t-1}</script> ， <script type="math/tex">\gamma</script> 是一个衰减系数。</li>
  <li>写入向量由之前的读取权重（代表着最近使用的位置）和之前的最少使用权重（代表着最少使用的位置）插值得到。插值参数是超参数 <script type="math/tex">\alpha</script> 的sigmoid。</li>
  <li>最少使用权重 <script type="math/tex">\mathbf{w}_t^{lu}</script> 由使用权重 <script type="math/tex">\mathbf{w}_t^u</script> 二值化得到。首先选取 <script type="math/tex">\mathbf{w}_t^u</script> 第n小的元素，找出 <script type="math/tex">\mathbf{w}_t^u</script> 中所有比这个元素小的元素，对应的位置在 <script type="math/tex">\mathbf{w}_t^{lu}</script> 中设为1，反之则设为0。</li>
</ol>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathbf{w}_t^u &= \gamma \mathbf{w}_{t-1}^u + \mathbf{w}_t^r + \mathbf{w}_t^w \\
\mathbf{w}_t^r &= \text{softmax}(\text{cosine}(\mathbf{k}_t, \mathbf{M}_t(i))) \\
\mathbf{w}_t^w &= \sigma(\alpha)\mathbf{w}_{t-1}^r + (1-\sigma(\alpha))\mathbf{w}^{lu}_{t-1}\\
\mathbf{w}_t^{lu} &= \mathbf{1}_{w_t^u(i) \leq m(\mathbf{w}_t^u, n)}
\text{, where }m(\mathbf{w}_t^u, n)\text{ is the }n\text{-th smallest element in vector }\mathbf{w}_t^u\text{.}
\end{aligned} %]]></script>

<p>最后，在把 <script type="math/tex">\mathbf{w}_t^{lu}</script> 代表着的最近使用存储设为0以后，更新每个存储向量：
 <script type="math/tex">\mathbf{M}_t(i) = \mathbf{M}_{t-1}(i) + w_t^w(i)\mathbf{k}_t, \forall i</script></p>

<h3 id="meta-networks">Meta Networks</h3>

<p><strong>Meta Networks</strong> (<a href="https://arxiv.org/abs/1703.00837">Munkhdalai &amp; Yu, 2017</a>)，简称<strong>MetaNet</strong>，是一个专门针对多任务间<em>快速</em>泛化设计的元学习模型，模型结构和训练过程都经过了调整。</p>

<h4 id="fast-weights">Fast Weights</h4>

<p>MetaNet的快速泛化能力依赖于“快参数（fast weights）”。有许多论文涉及这个话题，但我还没把他们全都仔细读过，没能找到一个具体的定义，只有一些模糊的共识。一般神经网络的权重是根据目标函数进行随机梯度下降更新的，但这个过程很慢。一种更快的学习方法是利用另外一个神经网络，预测当前神经网络的参数，预测出来的参数被称为<em>快参数</em>。而普通SGD生成的权重则被称为<em>慢参数</em>。</p>

<p>在MetaNet中，损失梯度作为<em>元信息</em>，被用于生产学习快参数的模型。慢参数和快参数在神经网络中被结合起来用于预测。快参数是针对任务进行优化产生的参数，使用快参数相当于针对当前的任务进行了优化。</p>

<p style="width: 50%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/combine-slow-fast-weights.png" alt="slow-fast-weights" /></p>
<p><em>Fig. 8. 结合了慢参数和快参数的MLP。 <script type="math/tex">\bigoplus</script> is element-wise sum. (图像来源：<a href="https://arxiv.org/abs/1703.00837">原论文</a>).</em></p>

<h4 id="model-components">Model Components</h4>

<blockquote>
  <p>免责声明：下面的部分与原始论文有所不同。在我看来，这篇文章的想法很有趣，但写的不太好，所以我想用自己的语言来介绍这篇文章。</p>
</blockquote>

<p>MetaNet的关键组件是：</p>
<ul>
  <li><script type="math/tex">f_\theta</script> ：一个由 <script type="math/tex">\theta</script> 决定的编码函数，发挥着元学习器的作用。负责把原始输入编码为特征向量。类似<a href="#convolutional-siamese-neural-network">Siamese Neural Network</a>，我们希望训练这个编码函数使得能够根据其生成的特征向量判断两个输入是否属于同一类（验证任务）。</li>
  <li><script type="math/tex">g_\phi</script> ：一个由 <script type="math/tex">\phi</script> 决定的基学习器，负责完成真正的学习任务。</li>
</ul>

<p>如果我们就此打住的话，这基本就是<a href="#relation-network">Relation Network</a>。
但MetaNet，给这两个模型额外增加了快参数（参加Fig. 8）。</p>

<p>因此，我们需要额外两个模型，分别用于生成 <script type="math/tex">f</script> 和 <script type="math/tex">g</script> 的快参数。</p>

<ul>
  <li><script type="math/tex">F_w</script> ：一个由 <script type="math/tex">w</script> 决定的LSTM，用于学习嵌入函数 <script type="math/tex">f</script> 的快参数 <script type="math/tex">\theta^+</script> 。它把 <script type="math/tex">f</script> 在验证任务上的loss梯度作为输入。</li>
  <li><script type="math/tex">G_v</script> ：一个由 <script type="math/tex">v</script> 决定的神经网络，根据基学习器 <script type="math/tex">g</script> 的loss梯度学习其快参数 <script type="math/tex">\phi^+</script> 。在MetaNet中，学习器的loss梯度被视作任务的<em>元信息</em>。</li>
</ul>

<p>现在我们来看看MetaNet是怎么训练的。训练数据包含许多对数据集。一个支持集 <script type="math/tex">S=\{\mathbf{x}'_i, y'_i\}_{i=1}^K</script> 和一个测试集 <script type="math/tex">U=\{\mathbf{x}_i, y_i\}_{i=1}^L</script> 。我们有四个模型和四个模型参数集 <script type="math/tex">(\theta, \phi, w, v)</script> 要学。</p>

<p style="width: 90%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/meta-network.png" alt="meta-net" /></p>
<p><em>Fig.9. MetaNet的结构。</em></p>

<h4 id="训练过程">训练过程</h4>

<ol>
  <li>从支持集 <script type="math/tex">S</script> 中随机采样一对输入， <script type="math/tex">(\mathbf{x}'_i, y'_i)</script> 和 <script type="math/tex">(\mathbf{x}'_j, y_j)</script> 。令 <script type="math/tex">\mathbf{x}_{(t,1)}=\mathbf{x}'_i</script> ， <script type="math/tex">\mathbf{x}_{(t,2)}=\mathbf{x}'_j</script> .<br />
for <script type="math/tex">t = 1, \dots, K</script> :
    <ul>
      <li>a. 计算编码函数的loss，即验证任务上的交叉熵<br />
<script type="math/tex">\mathcal{L}^\text{emb}_t = \mathbf{1}_{y'_i=y'_j} \log P_t + (1 - \mathbf{1}_{y'_i=y'_j})\log(1 - P_t)\text{, where }P_t = \sigma(\mathbf{W}\vert f_\theta(\mathbf{x}_{(t,1)}) - f_\theta(\mathbf{x}_{(t,2)})\vert)</script></li>
    </ul>
  </li>
  <li>根据编码函数的loss计算任务级别的快参数:
 <script type="math/tex">\theta^+ = F_w(\nabla_\theta \mathcal{L}^\text{emb}_1, \dots, \mathcal{L}^\text{emb}_T)</script></li>
  <li>接下来遍历支持集 <script type="math/tex">S</script> 中的样本，计算样本级别的快参数<br />
for <script type="math/tex">i=1, \dots, K</script> :
    <ul>
      <li>a. 基学习器输出一个概率分布： <script type="math/tex">P(\hat{y}_i \vert \mathbf{x}_i) = g_\phi(\mathbf{x}_i)</script> ，loss可以用交叉熵或者MSE: <script type="math/tex">\mathcal{L}^\text{task}_i = y'_i \log g_\phi(\mathbf{x}'_i) + (1- y'_i) \log (1 - g_\phi(\mathbf{x}'_i))</script></li>
      <li>b. 提取任务的元信息（loss的梯度）并计算样本级别的快参数:
<script type="math/tex">\phi_i^+ = G_v(\nabla_\phi\mathcal{L}^\text{task}_i)</script>
        <ul>
          <li>把 <script type="math/tex">\phi^+_i</script> 放在“值”存储器 <script type="math/tex">\mathbf{M}</script> 的第 <script type="math/tex">i</script> 个位置。<br /></li>
        </ul>
      </li>
      <li>d. 编码器结合慢参数和快参数对支持集中的样本进行编码： <script type="math/tex">r'_i = f_{\theta, \theta^+}(\mathbf{x}'_i)</script>
        <ul>
          <li>把 <script type="math/tex">r'_i</script> 放在“键”存储器 <script type="math/tex">\mathbf{R}</script> 的第 <script type="math/tex">i</script> 个位置。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>最后就是要来根据测试集 <script type="math/tex">U=\{\mathbf{x}_i, y_i\}_{i=1}^L</script> 来构建训练的loss了。注意，在这一步中，我们不使用 <script type="math/tex">F_w</script> 和 <script type="math/tex">G_v</script> 计算快参数。编码器的快参数保持不变，而基学习器的快参数则根据查询存储器得到。<br />
从 <script type="math/tex">\mathcal{L}_\text{train}=0</script> 开始：<br />
for <script type="math/tex">j=1, \dots, L</script> :
    <ul>
      <li>a. 使用编码器结合慢参数和之前得到的快参数对测试样例进行编码：
<script type="math/tex">r_j = f_{\theta, \theta^+}(\mathbf{x}_j)</script></li>
      <li>b. 运用注意力机制，查找编码得到的 <script type="math/tex">r_j</script> 在“键”存储器 <script type="math/tex">\mathbf{R}</script> 的位置，然后拿出对应位置“值”存储器 <script type="math/tex">\mathbf{M}</script> 对应位置的快参数，组合成当前测试样本对应的基学习器快参数。注意力函数可以随便选，MetaNet用的是cosine距离<br />
  <script type="math/tex">% <![CDATA[
\begin{aligned}
 a_j &= \text{cosine}(\mathbf{R}, r_j) = [\frac{r'_1\cdot r_j}{\|r'_1\|\cdot\|r_j\|}, \dots, \frac{r'_N\cdot r_j}{\|r'_N\|\cdot\|r_j\|}]\\
 \phi^+_j &= \text{softmax}(a_j)^\top \mathbf{M}
 \end{aligned} %]]></script></li>
      <li>c. 更新训练loss： <script type="math/tex">\mathcal{L}_\text{train} \leftarrow \mathcal{L}_\text{train} + \mathcal{L}^\text{task}(g_{\phi, \phi^+}(\mathbf{x}_i), y_i)</script></li>
    </ul>
  </li>
  <li>根据 <script type="math/tex">\mathcal{L}_\text{train}</script> 更新所有参数 <script type="math/tex">(\theta, \phi, w, v)</script> .</li>
</ol>

<h2 id="optimization-based">Optimization-Based</h2>

<p>Deep learning models learn through backpropagation of gradients. However, the gradient-based optimization is neither designed to cope with a small number of training samples, nor to converge within a small number of optimization steps. Is there a way to adjust the optimization algorithm so that the model can be good at learning with a few examples? This is what optimization-based approach meta-learning algorithms intend for.</p>

<h3 id="lstm-meta-learner">LSTM Meta-Learner</h3>

<p>The optimization algorithm can be explicitly modeled. <a href="https://openreview.net/pdf?id=rJY0-Kcll">Ravi &amp; Larochelle (2017)</a> did so and named it “meta-learner”, while the original model for handling the task is called “learner”. The goal of the meta-learner is to efficiently update the learner’s parameters using a small support set so that the learner can adapt to the new task quickly.</p>

<p>Let’s denote the learner model as <script type="math/tex">M_\theta</script> parameterized by <script type="math/tex">\theta</script> , the meta-learner as <script type="math/tex">R_\Theta</script> with parameters <script type="math/tex">\Theta</script> , and the loss function <script type="math/tex">\mathcal{L}</script> .</p>

<h4 id="why-lstm">Why LSTM?</h4>

<p>The meta-learner is modeled as a LSTM, because:</p>
<ol>
  <li>There is similarity between the gradient-based update in backpropagation and the cell-state update in LSTM.</li>
  <li>Knowing a history of gradients benefits the gradient update; think about how <a href="http://ruder.io/optimizing-gradient-descent/index.html#momentum">momentum</a> works.</li>
</ol>

<p>The update for the learner’s parameters at time step t with a learning rate <script type="math/tex">\alpha_t</script> is:</p>

<script type="math/tex; mode=display">\theta_t = \theta_{t-1} - \alpha_t \nabla_{\theta_{t-1}}\mathcal{L}_t</script>

<p>It has the same form as the cell state update in LSTM, if we set forget gate <script type="math/tex">f_t=1</script> , input gate <script type="math/tex">i_t = \alpha_t</script> , cell state <script type="math/tex">c_t = \theta_t</script> , and new cell state <script type="math/tex">\tilde{c}_t = -\nabla_{\theta_{t-1}}\mathcal{L}_t</script> :</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t\\
    &= \theta_{t-1} - \alpha_t\nabla_{\theta_{t-1}}\mathcal{L}_t
\end{aligned} %]]></script>

<p>While fixing <script type="math/tex">f_t=1</script> and <script type="math/tex">i_t=\alpha_t</script> might not be the optimal, both of them can be learnable and adaptable to different datasets.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
f_t &= \sigma(\mathbf{W}_f \cdot [\nabla_{\theta_{t-1}}\mathcal{L}_t, \mathcal{L}_t, \theta_{t-1}, f_{t-1}] + \mathbf{b}_f) & \scriptstyle{\text{; how much to forget the old value of parameters.}}\\
i_t &= \sigma(\mathbf{W}_i \cdot [\nabla_{\theta_{t-1}}\mathcal{L}_t, \mathcal{L}_t, \theta_{t-1}, i_{t-1}] + \mathbf{b}_i) & \scriptstyle{\text{; corresponding to the learning rate at time step t.}}\\
\tilde{\theta}_t &= -\nabla_{\theta_{t-1}}\mathcal{L}_t &\\
\theta_t &= f_t \odot \theta_{t-1} + i_t \odot \tilde{\theta}_t &\\
\end{aligned} %]]></script>

<h4 id="model-setup">Model Setup</h4>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/lstm-meta-learner.png" alt="lstm-meta-learner" /></p>
<p><em>Fig.10. How the learner <script type="math/tex">M_\theta</script> and the meta-learner <script type="math/tex">R_\Theta</script> are trained. (图像来源：<a href="https://openreview.net/pdf?id=rJY0-Kcll">原论文</a> with more annotations)</em></p>

<p>The training process mimics what happens during test, since it has been proved to be beneficial in <a href="#matching-networks">Matching Networks</a>. During each training epoch, we first sample a dataset <script type="math/tex">\mathcal{D} = (\mathcal{D}_\text{train}, \mathcal{D}_\text{test}) \in \hat{\mathcal{D}}_\text{meta-train}</script> and then sample mini-batches out of <script type="math/tex">\mathcal{D}_\text{train}</script> to update <script type="math/tex">\theta</script> for <script type="math/tex">T</script> rounds. The final state of the learner parameter <script type="math/tex">\theta_T</script> is used to train the meta-learner on the test data <script type="math/tex">\mathcal{D}_\text{test}</script> .</p>

<p>Two implementation details to pay extra attention to:</p>
<ol>
  <li>How to compress the parameter space in LSTM meta-learner? As the meta-learner is modeling parameters of another neural network, it would have hundreds of thousands of variables to learn. Following the <a href="https://arxiv.org/abs/1606.04474">idea</a> of sharing parameters across coordinates,</li>
  <li>To simplify the training process, the meta-learner assumes that the loss <script type="math/tex">\mathcal{L}_t</script> and the gradient <script type="math/tex">\nabla_{\theta_{t-1}} \mathcal{L}_t</script> are independent.</li>
</ol>

<p style="width: 100%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/train-meta-learner.png" alt="train-meta-learner" /></p>

<h3 id="maml">MAML</h3>

<p><strong>MAML</strong>, short for <strong>Model-Agnostic Meta-Learning</strong> (<a href="https://arxiv.org/abs/1703.03400">Finn, et al. 2017</a>) is a fairly general optimization algorithm, compatible with any model that learns through gradient descent.</p>

<p>Let’s say our model is <script type="math/tex">f_\theta</script> with parameters <script type="math/tex">\theta</script> . Given a task <script type="math/tex">\tau_i</script> and its associated dataset <script type="math/tex">(\mathcal{D}^{(i)}_\text{train}, \mathcal{D}^{(i)}_\text{test})</script> , we can update the model parameters by one or more gradient descent steps (the following example only contains one step):</p>

<script type="math/tex; mode=display">\theta'_i = \theta - \alpha \nabla_\theta\mathcal{L}^{(0)}_{\tau_i}(f_\theta)</script>

<p>where <script type="math/tex">\mathcal{L}^{(0)}</script> is the loss computed using the mini data batch with id (0).</p>

<p style="width: 45%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/maml.png" alt="MAML" /></p>
<p><em>Fig. 11. Diagram of MAML. (图像来源：<a href="https://arxiv.org/abs/1703.03400">原论文</a>)</em></p>

<p>Well, the above formula only optimizes for one task. To achieve a good generalization across a variety of tasks, we would like to find the optimal <script type="math/tex">\theta^*</script> so that the task-specific fine-tuning is more efficient. Now, we sample a new data batch with id (1) for updating the meta-objective. The loss, denoted as <script type="math/tex">\mathcal{L}^{(1)}</script> , depends on the mini batch (1). The superscripts in <script type="math/tex">\mathcal{L}^{(0)}</script> and <script type="math/tex">\mathcal{L}^{(1)}</script> only indicate different data batches, and they refer to the same loss objective for the same task.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta^* 
&= \arg\min_\theta \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta'_i}) = \arg\min_\theta \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta - \alpha\nabla_\theta \mathcal{L}_{\tau_i}^{(0)}(f_\theta)}) & \\
\theta &\leftarrow \theta - \beta \nabla_{\theta} \sum_{\tau_i \sim p(\tau)} \mathcal{L}_{\tau_i}^{(1)} (f_{\theta - \alpha\nabla_\theta \mathcal{L}_{\tau_i}^{(0)}(f_\theta)}) & \scriptstyle{\text{; updating rule}}
\end{aligned} %]]></script>

<p style="width: 60%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/maml-algo.png" alt="MAML Algorithm" /></p>
<p><em>Fig. 12. The general form of MAML algorithm. (图像来源：<a href="https://arxiv.org/abs/1703.03400">原论文</a>)</em></p>

<h4 id="first-order-maml">First-Order MAML</h4>

<p>The meta-optimization step above relies on second derivatives. To make the computation less expensive, a modified version of MAML omits second derivatives, resulting in a simplified and cheaper implementation, known as <strong>First-Order MAML (FOMAML)</strong>.</p>

<p>Let’s consider the case of performing <script type="math/tex">k</script> inner gradient steps, <script type="math/tex">k\geq1</script> . Starting with the initial model parameter <script type="math/tex">\theta_\text{meta}</script> :</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta_0 &= \theta_\text{meta}\\
\theta_1 &= \theta_0 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_0)\\
\theta_2 &= \theta_1 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_1)\\
&\dots\\
\theta_k &= \theta_{k-1} - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_{k-1})
\end{aligned} %]]></script>

<p>Then in the outer loop, we sample a new data batch for updating the meta-objective.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta_\text{meta} &\leftarrow \theta_\text{meta} - \beta g_\text{MAML} & \scriptstyle{\text{; update for meta-objective}} \\[2mm]
\text{where } g_\text{MAML}
&= \nabla_{\theta} \mathcal{L}^{(1)}(\theta_k) &\\[2mm]
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot (\nabla_{\theta_{k-1}} \theta_k) \dots (\nabla_{\theta_0} \theta_1) \cdot (\nabla_{\theta} \theta_0) & \scriptstyle{\text{; following the chain rule}} \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k \nabla_{\theta_{i-1}} \theta_i &  \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k \nabla_{\theta_{i-1}} (\theta_{i-1} - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1})) &  \\
&= \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k (I - \alpha\nabla_{\theta_{i-1}}(\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1}))) &
\end{aligned} %]]></script>

<p>The MAML gradient is:</p>

<script type="math/tex; mode=display">g_\text{MAML} = \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k) \cdot \prod_{i=1}^k (I - \alpha \color{red}{\nabla_{\theta_{i-1}}(\nabla_\theta\mathcal{L}^{(0)}(\theta_{i-1}))})</script>

<p>The First-Order MAML ignores the second derivative part in red. It is simplified as follows, equivalent to the derivative of the last inner gradient update result.</p>

<script type="math/tex; mode=display">g_\text{FOMAML} = \nabla_{\theta_k} \mathcal{L}^{(1)}(\theta_k)</script>

<h3 id="reptile">Reptile</h3>

<p><strong>Reptile</strong> (<a href="https://arxiv.org/abs/1803.02999">Nichol, Achiam &amp; Schulman, 2018</a>) is a remarkably simple meta-learning optimization algorithm. It is similar to MAML in many ways, given that both rely on meta-optimization through gradient descent and both are model-agnostic.</p>

<p>The Reptile works by repeatedly:</p>
<ul>
  <li>1) sampling a task,</li>
  <li>2) training on it by multiple gradient descent steps,</li>
  <li>3) and then moving the model weights towards the new parameters.</li>
</ul>

<p>See the algorithm below:
 <script type="math/tex">\text{SGD}(\mathcal{L}_{\tau_i}, \theta, k)</script> performs stochastic gradient update for k steps on the loss <script type="math/tex">\mathcal{L}_{\tau_i}</script> starting with initial parameter <script type="math/tex">\theta</script> and returns the final parameter vector. The batch version samples multiple tasks instead of one within each iteration. The reptile gradient is defined as <script type="math/tex">(\theta - W)/\alpha</script> , where <script type="math/tex">\alpha</script> is the stepsize used by the SGD operation.</p>

<p style="width: 52%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/reptile-algo.png" alt="Reptile Algorithm" /></p>
<p><em>Fig. 13. The batched version of Reptile algorithm. (图像来源：<a href="https://arxiv.org/abs/1803.02999">原论文</a>)</em></p>

<p>At a glance, the algorithm looks a lot like an ordinary SGD. However, because the task-specific optimization can take more than one step. it eventually makes <script type="math/tex">\text{SGD}(\mathbb{E}
_\tau[\mathcal{L}_{\tau}], \theta, k)</script> diverge from <script type="math/tex">\mathbb{E}_\tau [\text{SGD}(\mathcal{L}_{\tau}, \theta, k)]</script> when k &gt; 1.</p>

<h4 id="the-optimization-assumption">The Optimization Assumption</h4>

<p>Assuming that a task <script type="math/tex">\tau \sim p(\tau)</script> has a manifold of optimal network configuration, <script type="math/tex">\mathcal{W}_{\tau}^*</script> . The model <script type="math/tex">f_\theta</script> achieves the best performance for task <script type="math/tex">\tau</script> when <script type="math/tex">\theta</script> lays on the surface of <script type="math/tex">\mathcal{W}_{\tau}^*</script> . To find a solution that is good across tasks, we would like to find a parameter close to all the optimal manifolds of all tasks:</p>

<script type="math/tex; mode=display">\theta^* = \arg\min_\theta \mathbb{E}_{\tau \sim p(\tau)} [\frac{1}{2} \text{dist}(\theta, \mathcal{W}_\tau^*)^2]</script>

<p style="width: 50%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/reptile-optim.png" alt="Reptile Algorithm" /></p>
<p><em>Fig. 14. The Reptile algorithm updates the parameter alternatively to be closer to the optimal manifolds of different tasks. (图像来源：<a href="https://arxiv.org/abs/1803.02999">原论文</a>)</em></p>

<p>Let’s use the L2 distance as <script type="math/tex">\text{dist}(.)</script> and the distance between a point <script type="math/tex">\theta</script> and a set <script type="math/tex">\mathcal{W}_\tau^*</script> equals to the distance between <script type="math/tex">\theta</script> and a point <script type="math/tex">W_{\tau}^*(\theta)</script> on the manifold that is closest to <script type="math/tex">\theta</script> :</p>

<script type="math/tex; mode=display">\text{dist}(\theta, \mathcal{W}_{\tau}^*) = \text{dist}(\theta, W_{\tau}^*(\theta)) \text{, where }W_{\tau}^*(\theta) = \arg\min_{W\in\mathcal{W}_{\tau}^*} \text{dist}(\theta, W)</script>

<p>The gradient of the squared euclidean distance is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\nabla_\theta[\frac{1}{2}\text{dist}(\theta, \mathcal{W}_{\tau_i}^*)^2]
&= \nabla_\theta[\frac{1}{2}\text{dist}(\theta, W_{\tau_i}^*(\theta))^2] & \\
&= \nabla_\theta[\frac{1}{2}(\theta - W_{\tau_i}^*(\theta))^2] & \\
&= \theta - W_{\tau_i}^*(\theta) & \scriptstyle{\text{; See notes.}}
\end{aligned} %]]></script>

<p>Notes: According to the Reptile paper, “<em>the gradient of the squared euclidean distance between a point Θ and a set S is the vector 2(Θ − p), where p is the closest point in S to Θ</em>”. Technically the closest point in S is also a function of Θ, but I’m not sure why the gradient does not need to worry about the derivative of p. (Please feel free to leave me a comment or send me an email about this if you have ideas.)</p>

<p>Thus the update rule for one stochastic gradient step is:</p>

<script type="math/tex; mode=display">\theta = \theta - \alpha \nabla_\theta[\frac{1}{2} \text{dist}(\theta, \mathcal{W}_{\tau_i}^*)^2] = \theta - \alpha(\theta - W_{\tau_i}^*(\theta)) = (1-\alpha)\theta + \alpha W_{\tau_i}^*(\theta)</script>

<p>The closest point on the optimal task manifold <script type="math/tex">W_{\tau_i}^*(\theta)</script> cannot be computed exactly, but Reptile approximates it using <script type="math/tex">\text{SGD}(\mathcal{L}_\tau, \theta, k)</script> .</p>

<h4 id="reptile-vs-fomaml">Reptile vs FOMAML</h4>

<p>To demonstrate the deeper connection between Reptile and MAML, let’s expand the update formula with an example performing two gradient steps, k=2 in <script type="math/tex">\text{SGD}(.)</script> . Same as defined <a href="#maml">above</a>, <script type="math/tex">\mathcal{L}^{(0)}</script> and <script type="math/tex">\mathcal{L}^{(1)}</script> are losses using different mini-batches of data. For ease of reading, we adopt two simplified annotations: <script type="math/tex">g^{(i)}_j = \nabla_{\theta} \mathcal{L}^{(i)}(\theta_j)</script> and <script type="math/tex">H^{(i)}_j = \nabla^2_{\theta} \mathcal{L}^{(i)}(\theta_j)</script> .</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\theta_0 &= \theta_\text{meta}\\
\theta_1 &= \theta_0 - \alpha\nabla_\theta\mathcal{L}^{(0)}(\theta_0)= \theta_0 - \alpha g^{(0)}_0 \\
\theta_2 &= \theta_1 - \alpha\nabla_\theta\mathcal{L}^{(1)}(\theta_1) = \theta_0 - \alpha g^{(0)}_0 - \alpha g^{(1)}_1
\end{aligned} %]]></script>

<p>According to the <a href="#first-order-maml">early section</a>, the gradient of FOMAML is the last inner gradient update result. Therefore, when k=1:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= \nabla_{\theta_1} \mathcal{L}^{(1)}(\theta_1) = g^{(1)}_1 \\
g_\text{MAML} &= \nabla_{\theta_1} \mathcal{L}^{(1)}(\theta_1) \cdot (I - \alpha\nabla^2_{\theta} \mathcal{L}^{(0)}(\theta_0)) = g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1
\end{aligned} %]]></script>

<p>The Reptile gradient is defined as:</p>

<script type="math/tex; mode=display">g_\text{Reptile} = (\theta_0 - \theta_2) / \alpha = g^{(0)}_0 + g^{(1)}_1</script>

<p>Up to now we have:</p>

<p style="width: 50%;" class="center"><img src="/blog/assets/images/2019-09-19-meta-learning/reptile_vs_FOMAML.png" alt="Reptile vs FOMAML" /></p>
<p><em>Fig. 15. Reptile versus FOMAML in one loop of meta-optimization. (图像来源：<a href="https://www.slideshare.net/YoonhoLee4/on-firstorder-metalearning-algorithms">slides</a> on Reptile by Yoonho Lee.)</em></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= g^{(1)}_1 \\
g_\text{MAML} &= g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1 \\
g_\text{Reptile} &= g^{(0)}_0 + g^{(1)}_1
\end{aligned} %]]></script>

<p>Next let’s try further expand <script type="math/tex">g^{(1)}_1</script> using <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor expansion</a>. Recall that Taylor expansion of a function <script type="math/tex">f(x)</script> that is differentiable at a number <script type="math/tex">a</script> is:</p>

<script type="math/tex; mode=display">f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \dots = \sum_{i=0}^\infty \frac{f^{(i)}(a)}{i!}(x-a)^i</script>

<p>We can consider <script type="math/tex">\nabla_{\theta}\mathcal{L}^{(1)}(.)</script> as a function and <script type="math/tex">\theta_0</script> as a value point. The Taylor expansion of <script type="math/tex">g_1^{(1)}</script> at the value point <script type="math/tex">\theta_0</script> is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_1^{(1)} &= \nabla_{\theta}\mathcal{L}^{(1)}(\theta_1) \\
&= \nabla_{\theta}\mathcal{L}^{(1)}(\theta_0) + \nabla^2_\theta\mathcal{L}^{(1)}(\theta_0)(\theta_1 - \theta_0) + \frac{1}{2}\nabla^3_\theta\mathcal{L}^{(1)}(\theta_0)(\theta_1 - \theta_0)^2 + \dots & \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + \frac{\alpha^2}{2}\nabla^3_\theta\mathcal{L}^{(1)}(\theta_0) (g_0^{(0)})^2 + \dots & \scriptstyle{\text{; because }\theta_1-\theta_0=-\alpha g_0^{(0)}} \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>Plug in the expanded form of <script type="math/tex">g_1^{(1)}</script> into the MAML gradients with one step inner gradient update:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= g^{(1)}_1 = g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
g_\text{MAML} &= g^{(1)}_1 - \alpha H^{(0)}_0 g^{(1)}_1 \\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2) - \alpha H^{(0)}_0 (g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2))\\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + \alpha^2 \alpha H^{(0)}_0 H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
&= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>The Reptile gradient becomes:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{Reptile} 
&= g^{(0)}_0 + g^{(1)}_1 \\
&= g^{(0)}_0 + g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>So far we have the formula of three types of gradients:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
g_\text{FOMAML} &= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)\\
g_\text{MAML} &= g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} - \alpha H^{(0)}_0 g_0^{(1)} + O(\alpha^2)\\
g_\text{Reptile}  &= g^{(0)}_0 + g_0^{(1)} - \alpha H^{(1)}_0 g_0^{(0)} + O(\alpha^2)
\end{aligned} %]]></script>

<p>During training, we often average over multiple data batches. In our example, the mini batches (0) and (1) are interchangeable since both are drawn at random. The expectation <script type="math/tex">\mathbb{E}_{\tau,0,1}</script> is averaged over two data batches, ids (0) and (1), for task <script type="math/tex">\tau</script> .</p>

<p>Let,</p>
<ul>
  <li><script type="math/tex">A = \mathbb{E}_{\tau,0,1} [g_0^{(0)}] = \mathbb{E}_{\tau,0,1} [g_0^{(1)}]</script> ; it is the average gradient of task loss. We expect to improve the model parameter to achieve better task performance by following this direction pointed by <script type="math/tex">A</script> .</li>
  <li><script type="math/tex">B = \mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [H^{(1)}_0 g_0^{(0)} + H^{(0)}_0 g_0^{(1)}] = \frac{1}{2}\mathbb{E}_{\tau,0,1} [\nabla_\theta(g^{(0)}_0 g_0^{(1)})]</script> ; it is the direction (gradient) that increases the inner product of gradients of two different mini batches for the same task. We expect to improve the model parameter to achieve better generalization over different data by following this direction pointed by <script type="math/tex">B</script> .</li>
</ul>

<p>To conclude, both MAML and Reptile aim to optimize for the same goal, better task performance (guided by A) and better generalization (guided by B), when the gradient update is approximated by first three leading terms.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathbb{E}_{\tau,1,2}[g_\text{FOMAML}] &= A - \alpha B + O(\alpha^2)\\
\mathbb{E}_{\tau,1,2}[g_\text{MAML}] &= A - 2\alpha B + O(\alpha^2)\\
\mathbb{E}_{\tau,1,2}[g_\text{Reptile}]  &= 2A - \alpha B + O(\alpha^2)
\end{aligned} %]]></script>

<p>It is not clear to me whether the ignored term <script type="math/tex">O(\alpha^2)</script> might play a big impact on the parameter learning. But given that FOMAML is able to obtain a similar performance as the full version of MAML, it might be safe to say higher-level derivatives would not be critical during gradient descent update.</p>

<hr />

<p>Cited as:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{weng2018metalearning,
  title   = "Meta-Learning: Learning to Learn Fast",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "http://lilianweng.github.io/lil-log/2018/11/29/meta-learning.html"
}
</code></pre></div></div>

<p><em>If you notice mistakes and errors in this post, don’t hesitate to leave a comment or contact me at [lilian dot wengweng at gmail dot com] and I would be very happy to correct them asap.</em></p>

<p>See you in the next post!</p>

<h2 id="reference">Reference</h2>

<p>[1] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. <a href="https://www.cs.cmu.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf">“Human-level concept learning through probabilistic program induction.”</a> Science 350.6266 (2015): 1332-1338.</p>

<p>[2] Oriol Vinyals’ talk on <a href="http://metalearning-symposium.ml/files/vinyals.pdf">“Model vs Optimization Meta Learning”</a></p>

<p>[3] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. <a href="http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf">“Siamese neural networks for one-shot image recognition.”</a> ICML Deep Learning Workshop. 2015.</p>

<p>[4] Oriol Vinyals, et al. <a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">“Matching networks for one shot learning.”</a> NIPS. 2016.</p>

<p>[5] Flood Sung, et al. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">“Learning to compare: Relation network for few-shot learning.”</a> CVPR. 2018.</p>

<p>[6] Jake Snell, Kevin Swersky, and Richard Zemel. <a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">“Prototypical Networks for Few-shot Learning.”</a> CVPR. 2018.</p>

<p>[7] Adam Santoro, et al. <a href="http://proceedings.mlr.press/v48/santoro16.pdf">“Meta-learning with memory-augmented neural networks.”</a> ICML. 2016.</p>

<p>[8] Alex Graves, Greg Wayne, and Ivo Danihelka. <a href="https://arxiv.org/abs/1410.5401">“Neural turing machines.”</a> arXiv preprint arXiv:1410.5401 (2014).</p>

<p>[9] Tsendsuren Munkhdalai and Hong Yu. <a href="https://arxiv.org/abs/1703.00837">“Meta Networks.”</a> ICML. 2017.</p>

<p>[10] Sachin Ravi and Hugo Larochelle. <a href="https://openreview.net/pdf?id=rJY0-Kcll">“Optimization as a Model for Few-Shot Learning.”</a> ICLR. 2017.</p>

<p>[11] Chelsea Finn’s BAIR blog on <a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">“Learning to Learn”</a>.</p>

<p>[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. <a href="https://arxiv.org/abs/1703.03400">“Model-agnostic meta-learning for fast adaptation of deep networks.”</a> ICML 2017.</p>

<p>[13] Alex Nichol, Joshua Achiam, John Schulman. <a href="https://arxiv.org/abs/1803.02999">“On First-Order Meta-Learning Algorithms.”</a> arXiv preprint arXiv:1803.02999 (2018).</p>

<p>[14] <a href="https://www.slideshare.net/YoonhoLee4/on-firstorder-metalearning-algorithms">Slides on Reptile</a> by Yoonho Lee.</p>


  </div>


  <div class="page-navigation">
    
      <a class="prev" href="/blog/%E8%AE%BA%E6%96%87/2017/06/04/OSVOS%E8%B7%9F%E8%BF%9B.html">&larr; OSVOS跟进</a>
    

    
      <a class="next" href="/blog/2019/09/23/gaussian-process.html">高斯过程回归科普 &rarr;</a>
    
  </div>

  
    <!-- <div id="disqus_thread"></div>
<script>
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://lilianweng-github-io-lil-log.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<noscript>
    Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
 -->
  

</article>

      </div>
    </main>

    <div style="clear: both;"/>
<footer class="site-footer">
    2019 &copy; Built by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> and <a href="https://github.com/jekyll/minima/" target="_blank">minima</a> | View <a href="https://github.com/wei-tianhao/blog" target="_blank">this</a> on Github | <a href="/blog/tags.html">Tags</a> | <a href="/blog/contact.html">Contact</a> | <a href="/blog/FAQ.html">FAQ</a>

    <p>
        <a href="/blog/feed.xml" target="_blank">
            <img src="/blog/assets/images/logo_rss.png" />
        </a>
        <a href="https://scholar.google.com/citations?user=V22j1C0AAAAJ&hl=en" target="_blank">
            <img src="/blog/assets/images/logo_scholar.png" />
        </a>
        <a href="https://github.com/wei-tianhao/" target="_blank">
            <img src="/blog/assets/images/logo_github.png" />
        </a>
        <!-- <a href="https://www.instagram.com/lilianweng/" target="_blank">
            <img src="/blog/assets/images/logo_instagram.png" />
        </a> -->
        <!-- <a href="https://twitter.com/lilianweng/" target="_blank">
            <img src="/blog/assets/images/logo_twitter.png" />
        </a> -->
    </p>
</footer>


  </body>

</html>
