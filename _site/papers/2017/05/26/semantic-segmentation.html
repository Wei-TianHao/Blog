<!DOCTYPE html>
<html lang="en">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>语义分割笔记</title>
  <meta name="description" content="语义分割笔记">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/papers/2017/05/26/semantic-segmentation.html">
  <link rel="alternate" type="application/rss+xml" title="My blog" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">My blog</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">语义分割笔记</h1>
    <p class="post-meta">
      <time datetime="2017-05-26T10:00:42+08:00" itemprop="datePublished">
        
        May 26, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h1 id="语义分割笔记">语义分割笔记</h1>

<h3 id="image-caption为什么需要semantic-segmentation">Image Caption为什么需要Semantic Segmentation</h3>

<p>一开始的网络只是把CNN的FC层直接输入RNN，但这个层里面的东西是难以解释的，但是RNN这么稀里糊涂的弄一弄就能描述出来图片了。这让人非常没有掌控感，于是后来Google有一篇论文就是讨论输入RNN的东西的可解释性是否对于Image caption有作用，一个很自然的想法是不仅要输入图像中的各项特征，而最好能把图像中的各个物体标注出来，将语义信息输入RNN。结果发现，输入可解释的信息大大提高了神经网络的表现。并不是稀里糊涂的一通训练就可以得到好的效果的。这篇论文非常具有启发性。一个创新之后，对这个创新中的局部进行优化，对局部之间的协作方式进行优化，对创新中说得不清晰或者不合理的部分敢于反思并探索，往往大的提升就在这些模糊的区域中了。接下来是几篇经典论文串讲，从最基础的AlexNet开始。</p>

<h4 id="imagenet-classification-with-deep-convolutional-neural-networks">ImageNet Classification with Deep Convolutional Neural Networks</h4>

<p>这篇论文开创了利用深度卷积神经网络进行图像识别的方法。也就是著名的AlexNet，结构如下图，5个卷积层，3个全连接层：</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/044A5076-2F92-4F13-BC6C-B16399096979.png" alt="044A5076-2F92-4F13-BC6C-B16399096979" /></p>

<p>虽然AlexNet不是CNN的开创者，但他使用了许多技术使得CNN的识别能力大幅提高并已成为现在的标准配置，有</p>

<ol>
  <li>ReLU：没有饱和的问题，更快</li>
  <li>Overlapping Pooling，轻微改善，防止过拟合</li>
  <li>多GPU并行, 更快</li>
  <li>LRN，ReLU后的局部归一化，虽然ReLU对很大的X依然有效，但这样还是能改善一些</li>
  <li>减少过拟合：1）数据扩增，各种形态学变化之类的。 2）Dropout，方便好用，记得test的时乘上</li>
  <li>Weight Decay，感觉实际上就是正则项$\lambda$</li>
</ol>

<p>接下来介绍如何用CNN作语义分割</p>

<h4 id="fully-convolutional-networks-for-semantic-segmentation">Fully Convolutional Networks for Semantic Segmentation</h4>

<p>这篇论文最早提出了全卷积网络的概念，想法其实很简单，CNN的输出是一维的向量，如果我们把最后面的FC层全都换成卷积层，就可以输出二维向量了，下图就是AlexNet卷积化后形成的全卷积网络：</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/AD9967C8-2C82-411F-9322-FCF3743CF1C4.png" alt="AD9967C8-2C82-411F-9322-FCF3743CF1C4" /></p>

<p>而且因为FCN与CNN结构非常相似，任务也比较接近，可以利用CNN训练好的网络进行Fine tuning，节省训练时间。而且在计算卷积的时候因为receptive fields重叠的非常多，所以训练很高效（这里不是很懂。。）</p>

<p>但从图中可以看出，这样最终生成的图像是比原来小的，而语义分割需要得到与原图同样大小的图像，那怎么办呢？接着论文提出了upsampling，deconvolution（CS231n里讲这个名字被吐槽的很多，叫conv transpose之类的比较好）的技巧（本质就是插值）。Deconvolution实际上就是将卷积的正向传播和反向传播反过来。反向卷积能否学习对于表现没有明显提升，所以学习率被置零了。但deconvolution又带来了一个问题，就是分辨率的问题，很容易想象出来，好比一张小照片被放大了一样，非常模糊。为了解决这个问题，作者又提出了skip layer的方法，即将前面的卷积层与后面同样大小的反卷积层结合起来。</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/1705275F-792B-4BA4-8A47-72B219882490.png" alt="1705275F-792B-4BA4-8A47-72B219882490" /></p>

<h3 id="deeplab-semantic-image-segmentation-withdeep-convolutional-nets-atrous-convolutionand-fully-connected-crfs">DeepLab: Semantic Image Segmentation withDeep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs</h3>

<p>这篇论文提出了使用DCNN实现语义分割的3个主要挑战</p>

<ol>
  <li>DCNN降低了特征的分辨率，而且为了保证图片不太小加入了100的padding，引入了噪声</li>
  <li>图片上存在着大小不一的物体</li>
  <li>图片特征在DCNN中的空间变化不变性导致的细节丢失（局部精确性与分类准确性的矛盾，上一篇论文使用了skip layer来处理这个问题）</li>
</ol>

<h5 id="第一个问题">第一个问题</h5>

<p>作者首先更改了最后两层池化层，把pooling的stride改为1，同时加上1个padding，这样池化后像素的个数就不再改变了。</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/766fc04b86b72f7e09d8f8ff6cb648e2_r-1.png" alt="766fc04b86b72f7e09d8f8ff6cb648e2_r" /></p>

<p>上图的a是原来的池化，b是更改后的池化，c是为了增加感受野带洞的卷积atrous conv。（<strong>==这里池化和卷积分的不太清楚，之后看下代码==</strong>）为什么要带洞呢，是因为b图的感受野是比a要小的，可以看出b图中池化后的连续三个像素对应着池化前的5个，而a图则对应着7个。这会导致全局性的削弱。因此作者收到atrous算法的启发，加上了洞。在扩大分辨率的同时保持了感受野。更改后输出的预测图的大小是原来的4倍，下图直观展示了效果，下图是先将一张图片downsample为1/2，然后分别使用竖向高斯导数卷积核和atrous核，最后再upsampling，高斯核只能得到原图的1/4坐标的预测，而atrous核能得到全部像素的预测<img src="/assets/2017-05-26-semantic-segmentation/屏幕快照 2017-05-26 上午11.05.02.png" alt="屏幕快照 2017-05-26 上午11.05.02" /></p>

<p>atrous具体的实现方法有两种，一种是往卷积核里插0，一种是把图片subsample，然后再标准卷积</p>

<p>有一点要说一下，为什么不把池化层直接去掉呢？主要是因为去掉以后网络结构改变，没法使用训练好的网络fine tuning。因为图像识别的数据量比较大，网络训练的比较成熟，所以一般都希望能够借助其训练好的模型。</p>

<h5 id="第二个问题">第二个问题</h5>

<p>不同尺寸目标的问题。一个好的解决方案是对于不同尺寸分别做DCNN，但这样太慢了，所以作者用了ASPP，并行的使用多个rate不同的atrous conv，这些卷积核共享参数，所以训练快了很多。如下图所示。</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/431DA9CC-7296-4224-B087-6FD7482B8733.png" alt="431DA9CC-7296-4224-B087-6FD7482B8733" /></p>

<h5 id="第三个问题">第三个问题</h5>

<p>局部精确性与分类表现的矛盾问题。作者说有两种解决方法，一种就是利用多层网络中的信息来增强细节，如skip layers；另一种就是使用一些super-pixel（把像素划分成区域）表示，直接去底层获取信息，比如CRF</p>

<p>CRF的一个101http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/（下面写了个3分钟版本的CRF感想，这个以后还有再系统学一下）</p>

<p>简单概括来说，CRF就是对于一个给定的全局观测，许多设定的特征函数，计算一个标签序列在这些特征函数下的得分，然后加权求和求得这个标签序列的得分。再将所有标签序列的得分Softmax归一化，作为该序列的概率。</p>

<h4 id="-scorels--sigma_j1msigma_i1nlambda_jf_js-i-l_i-l_i-1-">$ score(l|s) = \Sigma_{j=1}^{m}\Sigma_{i=1}^n\lambda_jf_j(s, i, l_i, l_{i-1}) $</h4>

<h4 id="pls--fracexpscorelssigma_l-expscorels">$p(l|s) = \frac{exp[score(l|s)]}{\Sigma_{l’} exp[score(l’|s)]}$</h4>

<p>$f_j$是特征函数，具体定义由问题决定（比如在词义分析中，可以定义为形容词后面是名词则$f$为1，否则为0），$l$是一个标签序列，这里的公式针对的是一维的情况，在图像标注中应该改成二维的，$l_{i-1}$在二维中对应着$i$的邻居节点的标签</p>

<p>要做的事情就是学习$\lambda_j$的值，这跟Logistics回归非常像，实际上这就是个时间序列版的logistics回归。一般目标是用最大似然估计来衡量学习。</p>

<p>每一个HMM（隐马尔科夫模型）都等价于一个CRF，就是说CRF比HMM更强。对HMM模型取对数之后吧概率对数看做权值，即化为CRF。这是因为CRF的特征函数具有更强的自由性，可以根据全局来定义特征函数，而HMM自身带有局部性，限制了其相应的特征函数。而且CRF可以使用任意权重，而HMM只能使用对数概率作为权重。</p>

<p>在这篇论文中，优化的目标是使下面这个函数最小</p>

<h4 id="ex--sigma_itheta_ix_isigma_ijtheta_ijx_i-x_j">$E(x) = \Sigma_i\theta_i(x_i)+\Sigma_{ij}\theta_{ij}(x_i, x_j)$</h4>

<p>$x_i$是第$i$个像素的标签，$\theta_i(x_i) = -log P(x_i)$，$P(x_i)$是第i个像素贴上$x_i$这个标签的概率（由DCNN算出来的），$\theta_{ij}(x_i, x_j)$是像素$i$像素$j$之间关系的度量</p>

<h4 id="theta_ijx_i-x_j--mux_i-x_jw_1-exp-fracp_i-p_j22sigma_alpha2-fraci_i-i_j22sigma_beta2-----------------------------------w_2-exp-fracp_i-p_j22sigma_gamma2">$\theta_{ij}(x_i, x_j) = \mu(x_i, x_j)[w_1\ exp(-\frac{||p_i-p_j||^2}{2\sigma_{\alpha}^2}-\frac{||I_i-I_j||^2}{2\sigma_{\beta}^2}) \\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + w_2\ exp(-\frac{||p_i-p_j||^2}{2\sigma_{\gamma}^2})]$</h4>

<p>$\mu$在$x_i, x_j$相等的时候是0，不相等时是1（只会惩罚相同标签的像素），这就是个双边滤波……</p>

<h5 id="实验中有启发的几个点">实验中有启发的几个点</h5>

<ol>
  <li>
    <p>learning rate使用poly策略比较好</p>
  </li>
  <li>
    <p>batch size小一点（最后取了10），迭代次数多一点更有利于训练</p>
  </li>
  <li>
    <p>在PASCAL-Person-Part上训练的时候LargeFOV和ASPP对于训练效果都没有提升，但CRF的提升效果非常明显。技术有适用性吧，No free lunch theory.</p>
  </li>
  <li>
    <p>从结果上来看CRF好像做了一些平滑和去噪的工作。（<strong>==对于CRF理解还不太到位，之前感觉像是起到精细化的作用，这里主要是双边滤波在起作用？==</strong>）
<img src="/assets/2017-05-26-semantic-segmentation/D93EE4D0-6893-45C5-B6D6-65E608C01E7B.png" alt="D93EE4D0-6893-45C5-B6D6-65E608C01E7B" /></p>

    <p><img src="/assets/2017-05-26-semantic-segmentation/270A5970-D3CE-4C4D-86C0-86BC9E526AA3.png" alt="270A5970-D3CE-4C4D-86C0-86BC9E526AA3" />
<img src="/assets/2017-05-26-semantic-segmentation/E19801B2-2B12-443A-BB57-C9786F9AFF16.png" alt="E19801B2-2B12-443A-BB57-C9786F9AFF16" /></p>
  </li>
  <li>
    <p>Cityscapes的图片非常大，作者一开始先缩小了一半再训练的，但后来发现用原始大小的图片训练能提高1.9%，效果很明显（但我感觉缩小一半对于细节的损失并不是很大因为原始图片有2048*1024，可能是因为训练量上升了？）。作者的处理方法是把原始图片分割成几张有重叠区域的图片再训练，训练好了拼起来。</p>
  </li>
  <li>
    <p>Failure Modes，作者发现他们的模型难以抓住复杂的边界，如下图，甚至可能会被CRF完全抹掉，因为因为DCNN算出来的东西不够自信（零星、稀疏）。作者看好encoder-decoder结构能够解决这个问题
<img src="/assets/2017-05-26-semantic-segmentation/E7E2AC8F-283B-4374-B92F-2928E8E0C857.png" alt="E7E2AC8F-283B-4374-B92F-2928E8E0C857" /></p>
  </li>
</ol>

<h3 id="faster-r-cnntowards-real-time-object-detection-with-region-proposal-networks">Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks</h3>

<p>目标检测近来的发展得益于region proposal methods和region-based convolutional nueral networks的成功。RPM负责给出粗略的语义分割，而R-CNN负责精细化的检测。Fast R-CNN已经得到了几乎实时的运行时间，而现在瓶颈就在于计算RPM，本文的目标就是使用RPN来突破该瓶颈，达到实时目标检测。这篇论文提出了RPN代替了常用的Region proposal methods,负责给出粗略的语义分割。</p>

<p>主要的原理是共享卷积层。作者们发现region-based detectors（比如Fast R-CNN）使用的卷积层产生的特征，也可以用来生成region proposals。</p>

<h5 id="rpn的构建">RPN的构建</h5>

<p>为了共享卷积，作者考察了ZF model（5层共享卷积）和SZ model（VGG，13层共享卷积层）</p>

<p>为了生成region proposals，作者在最后一个共享卷积层输出的特征层上做slide window。把一个window里的通过一个全连接层，生成一个低维向量。这个向量接着再被喂进两个平行的全连接层，分别用于矩形定位和矩形分类打分。</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/E31B36A8-B635-46F8-A51D-F7154C75DDC8.png" alt="E31B36A8-B635-46F8-A51D-F7154C75DDC8" /></p>

<p>实际上这个slide window就是个卷积，后面的两层也是卷积层。对于每个window会提出k个region proposal。作者说这个方法有个很重要的属性是translation invariant（平移不变性，平移后仍能预测出相同大小的anchor boxes）。</p>

<h5 id="rpn的学习过程">RPN的学习过程</h5>

<p>有着最大IOU或与所有goud-truth box 的IOU都大于70%的anchor会被赋予正标签；</p>

<p>与所有ground-truth box的IOU都小于30%的anchor会被赋予负例；</p>

<p>其他的anchor不会对训练有贡献。Loss function如下</p>

<h4 id="lp_i-t_i--frac1n_clssigma_il_clsp_i-p_i--lambda-frac1n_reg-sigma_i-p_i-l_regt_i-t_i">$L(p_i, t_i) = \frac1{N_{cls}}\Sigma_iL_{cls}(p_i, p_i^<em>) + \lambda \frac1{N_{reg}} \Sigma_i p_i^</em> L_{reg}(t_i, t_i^*)$</h4>

<p>i是anchor的index，$p_i$是anchor i被预测为是一个物体的概率，$p^<em>_i$是ground-truth（如果anchor是positive则为1，否则为0），$t_i$是表示box四个坐标的参数向量，$t^</em><em>I$是ground-truth。$L</em>{cls}$是log loss（Softmax分类器)，$L_{reg}(t_i, t_i^<em>)=R(t_i - t_i^</em>)$，$R$是robust loss function(smooth L1) (==<strong>这是啥</strong>==)。因为$p^*_i$，第二项只有正例的时候才会起作用。$\lambda$是一个平衡系数。</p>

<h5 id="优化">优化</h5>

<p>每个mini-batch都来自于同一张图片，随机取128个正例和128个负例，如果不够128个正例，就用负例填上</p>

<h5 id="共享卷积特征">共享卷积特征</h5>

<p>共享卷积特征存在这一个困难，Fast R-CNN的训练是基于固定的region proposals的，所以没法直接训练联合模型。而且不知道联合训练是否能让共享卷积层收敛。所以作者提出了按如下步骤训练的方法。</p>

<ol>
  <li>训练RPN，用ImageNet预训练的模型fine-tune。</li>
  <li>训练Fast R-CNN，使用第1步中RPN生成的proposals，到现在为止没有共享卷积层</li>
  <li>用Fast R-CNN初始化RPN的训练，但是只fine-tune RPN自己的层，不更改共享的卷积层</li>
  <li>fine-tune Fast R-CNN的fc层，也不更改共享的卷积层</li>
</ol>

<p>也就是说卷积层没被联合训练过。</p>

<h5 id="实现细节">实现细节</h5>

<p>许多RPN proposals高度重叠，作者使用了名为non-maximum suppression（NMS）的技术，NMS大大降低了proposal的数量而没有损害检测精度。</p>

<h5 id="实验">实验</h5>

<p>下面是几种技术对于结果的影响的比较</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/屏幕快照 2017-05-26 上午11.14.58.png" alt="屏幕快照 2017-05-26 上午11.14.58" /></p>

<h3 id="deformable-convolutional-networks">Deformable Convolutional Networks</h3>

<p>视觉识别中一个很大的问题在于图像的变形（角度、大小、姿势等）。以往的训练都是通过增加数据来使网络熟悉各种变形或使用一些形变时不变的特征（像是SIFT, scale invariant feature transform）。这篇论文提出CNN需要专门针对变形的结构才能较好的解决这个问题，因此提出了deformable convolution。</p>

<h5 id="deformable-convolution">Deformable Convolution</h5>

<p>基本思想是改变卷积层的核，原来核是一个方形，现在对于核中每个元素加上一个offset，卷积后的特征不再来源于一个方形，而可能来源于各种形状。</p>

<p><img src="/assets/2017-05-26-semantic-segmentation//Users/wth/Desktop/沟通技巧/屏幕快照 2017-05-26 下午1.20.54.png" alt="屏幕快照 2017-05-26 下午1.20.54" /></p>

<p>原来的卷积公式是这样子：</p>

<h4 id="yp_0--sigma_p_n-wp_n-cdot-xp_0p_n">$y(p_0) = \Sigma_{p_n} w(p_n) \cdot x(p_0+p_n)$</h4>

<p>加上偏移量$\Delta p_n$后变成这个样子：</p>

<h4 id="yp_0--sigma_p_n-wp_n-cdot-xp_0p_n-delta-p_n">$y(p_0) = \Sigma_{p_n} w(p_n) \cdot x(p_0+p_n+ \Delta p_n)$</h4>

<p>但因为偏移量常常是小数，所以要用双线性插值找到偏移后的坐标最接近的整数位置，公式略。</p>

<h4 id="deformable-roi-pooling">Deformable RoI Pooling</h4>

<p>RoI pooling是将一个任意大小的图片转化为固定大小输出的池化。池化函数是bin内的平均值。原始公式是</p>

<h4 id="yij--sigma_p-xp_0--p--n_ij">$y(i,j) = \Sigma_{p} x(p_0 + p) / n_{ij}$</h4>

<p>$p_0$是bin的左上角，p是枚举位置，$n_{ij}$是bin内的元素总数， 加上偏移量后</p>

<h4 id="yij--sigma_p-xp_0--p-delta-p_ij--n_ij">$y(i,j) = \Sigma_{p} x(p_0 + p +\Delta p_{ij}) / n_{ij}$</h4>

<p>偏移量的学习学习的是相对系数（图片大小的百分比），这样能够适用于不同大小的图片。</p>

<p>还可以扩展到position-sensitive RoI pooling，<strong>==（这里不太清楚，以后再看）==</strong></p>

<h4 id="理解deformable-convnets">理解Deformable ConvNets</h4>

<p>下图是使用了的deformable conv后感受野的变化</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/屏幕快照 2017-05-26 下午8.27.21.png" alt="屏幕快照 2017-05-26 下午8.27.21" /></p>

<p>而且因为核具有自己调整的特性，可以轻松识别出不同scale的物体，下图展示了这一特性，每张图片中的红点是三层卷积对应的感受野，绿点是最高层的中心</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/屏幕快照 2017-05-26 下午8.30.40.png" alt="屏幕快照 2017-05-26 下午8.30.40" /></p>

<p>对于RoI也是类似的效果，黄框的分数是由红框的平均值计算来的</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/屏幕快照 2017-05-26 下午8.30.49.png" alt="屏幕快照 2017-05-26 下午8.30.49" /></p>

<h4 id="与相关工作的对比">与相关工作的对比</h4>

<p>有几个有趣的点</p>

<ol>
  <li>Effective Receptive Field这里提到，感受野虽然理论上随着层数线性增长，但实际上是成根号增长的，比预期的慢很多，因此即使是顶层的单元感受野也很小。因此Atrous Conv由于其有效增加感受野得到了广泛的应用</li>
  <li>之前也有动态filter的研究，但都只是值的变化而不是位置的变化</li>
  <li>当多层卷积结合起来以后，可能会有着跟deformable conv类似的效果，但存在着本质上的不同。经过复杂学习后得到的东西如果换一种思考方式就变得意外简单。</li>
</ol>

<h4 id="实验-1">实验</h4>

<p>几种网络应用了deformable conv后的效果比较：</p>

<p><img src="/assets/2017-05-26-semantic-segmentation/屏幕快照 2017-05-26 下午8.59.34.png" alt="屏幕快照 2017-05-26 下午8.59.34" /></p>

<p>不知道为什么Faster R-CNN的提升效果最差（可能是RPN），而DeepLab应用6层deformable conv后效果反而变差了（猜测是感受野过大，容易分散，或在某些特征点收敛，过于集中，太关注于局部信息）</p>

<h4 id="aligned-inception-resnet">Aligned-Inception-ResNet</h4>

<p><strong>==这个网络还需要学习一下==</strong></p>

<h4 id="感想">感想</h4>

<p>感觉这篇论文的想法非常秒，很优雅。在知乎上看到一句话，ALAN Huang说的，感觉非常有启发性</p>

<blockquote>
  <p>conv，pooling这种操作，其实可以分成三阶段： indexing（im2col） ，reduce(sum), reindexing（col2im). 在每一阶段都可以做一些事情。 用data driven的方式去学每一阶段的参数，也是近些年的主流方向。</p>
</blockquote>


  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">My blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              My blog
            
            </li>
            
            <li><a href="mailto:phi.wth@gmail.com">phi.wth@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/Wei-TianHao"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">Wei-TianHao</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>都是骗人的
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
