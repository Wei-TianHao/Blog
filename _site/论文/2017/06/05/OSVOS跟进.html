<!DOCTYPE html>
<html lang="en">
  <script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js"></script>  
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>OSVOS跟进</title>
  <meta name="description" content="Semantically-Guided Video Object Segmentation">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://wei-tianhao.github.io/%E8%AE%BA%E6%96%87/2017/06/05/OSVOS%E8%B7%9F%E8%BF%9B.html">
  <link rel="alternate" type="application/rss+xml" title="My blog" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">My blog</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  
  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">OSVOS跟进</h1>
    <p class="post-meta">
      <time datetime="2017-06-05T10:00:00+08:00" itemprop="datePublished">
        
        Jun 5, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="semantically-guided-video-object-segmentation">Semantically-Guided Video Object Segmentation</h2>

<p>这篇论文我很喜欢，更符合人类的认知过程。</p>

<p>该篇论文提出的方法是模拟人类在视频中追踪物体的情形，人们在视频追踪的时候分为两种情形，一种是连续的画面，那很自然的就由上一帧的物体所在点过渡过来；但是当漏了几秒没看的时候，人们是怎么识别物体的呢？这就是该篇论文的出发点，语义分析追踪。即我第一帧看到了车，在画面不能连续起来的时候我就去找“车”这个语义在图片哪里。</p>

<p>对于第一帧图片使用FCN对图片中的各种物体做出像素级预测，然后寻找与mask重合最多的预测，比如说是car。对后面的帧预测的时候，即可先对图片做语义分割，然后找语义为car的预测，在于上一帧的mask结合，做出预测。总体结构如下<img src="/assets/2017-06-05-OSVOS跟进/屏幕快照 2017-06-05 上午12.15.02.png" alt="屏幕快照 2017-06-05 上午12.15.02" /></p>

<p>论文还提出了一个conditional classifier layer，主要功能是视情况结合propagation的结果和semantic segmentation的结果。比如物体移动非常剧烈的时候只采用semantic segmentation，放弃propagation的mask；而又多个相同语义的物体时则要侧重于propagation的结果（具体实现以后还要再看下）。</p>

<p><img src="/assets/2017-06-05-OSVOS跟进/屏幕快照 2017-06-05 上午12.17.36.png" alt="屏幕快照 2017-06-05 上午12.17.36" /></p>

<p>这篇论文我觉的最符合人类的直观认识，不知道还能不能再从这方面深入挖掘一下。</p>

<h2 id="lucid-data-dreaming-for-object-tracking">Lucid Data Dreaming for Object Tracking</h2>

<p>该篇论文主要提出了一种增强数据的方法，可以只用训练集里的数据就达到较好的效果。</p>

<p>按照以下五步来</p>

<ol>
  <li>光照随机变化，变换HSV中S和V</li>
  <li>把前景抠出来，补全背景</li>
  <li>随机移动、变形前景</li>
  <li>随机模拟相机变化，平移、旋转、放缩</li>
  <li>前景背景结合</li>
</ol>

<p><img src="/assets/2017-06-05-OSVOS跟进/屏幕快照 2017-06-04 下午11.54.18.png" alt="屏幕快照 2017-06-04 下午11.57.37" /></p>

<p>作者用一帧生成<script type="math/tex">10^3</script>级别的训练数据，效果相同的情形下数据量仅为原来的<script type="math/tex">\frac1{100}</script>到<script type="math/tex">\frac1{20}</script>。这种数据生成是跟网络完全独立的，可以用在以后的训练中。</p>

<p>作者训练用的模型是结合上一帧的mask与optical flow的模型，不是本文研究的重点，简要介绍了一下。</p>

<p><img src="/assets/2017-06-05-OSVOS跟进/屏幕快照 2017-06-04 下午11.57.37.png" alt="屏幕快照 2017-06-04 下午11.57.37" /></p>

<h2 id="learning-video-object-segmentation-from-static-images">Learning Video Object Segmentation from Static Images</h2>

<p>这篇论文提出将视频vido object segmentation看做是guided instance segmentation。本文的模型是先用静态图像预训练convnet，再由视频中的前几帧引导，生成高精确度的分割。</p>

<p><img src="/assets/2017-06-05-OSVOS跟进/屏幕快照 2017-06-03 下午12.08.47.png" alt="屏幕快照 2017-06-03 下午12.08.47" /></p>

<p>模型的关键在于离线和在线算法的结合，离线算法用于学习物体的特征，在线算法refine mask。大步骤跟OSVOS基本一致，但本质思想不同</p>

<h3 id="与osvos的区别">与OSVOS的区别</h3>

<ol>
  <li>
    <p>总体思路是Mask Track，而OSVOS则是Mask再识别。对于当前帧的预测，该篇论文使用当前帧帧的前几帧做引导，但OSVOS只是用了视频的第一帧，即没有propagation的过程。只用第一帧可能会导致效果随着时间下降（与第一帧差异越来越大）。</p>
  </li>
  <li>
    <ul>
      <li>第一步pre-training，同样是图像识别</li>
      <li>第二步offline training，OSVOS是使用训练集使网络学习mask的广义概念，而该篇则注重使网络学习如何propagating（根据前几帧的mask和当前帧推导出当前帧的mask）</li>
      <li>第三步online training，同样是使用test视频的一张标注来refine，而OSVOS还有轮廓的CNN预测来提高精确度。该篇的refine是通过对第一张mask进行各种变换形成许多训练数据，用这些数据训练网络，在test时用第一张标注辅助propagation（类似广义mask）</li>
      <li>Test，OSVOS只用第一张进行mask预测，该篇除了使用propagation以外也同样将第一张标注用于所有图像的mask预测</li>
    </ul>

    <p>主要区别在于第二步，第三步中OSVOS的轮廓预测是独立的模块，可以应用到该篇</p>
  </li>
</ol>

<h3 id="训练细节">训练细节</h3>

<p>使用的网络是DeepLabv2-VGG network，</p>

<p>第二步的实际训练方式是先将前一帧的mask做一些形态学变换模拟各种噪声，增大数据量，同时使用图片识别的mask进行一些形态学变换，来模拟前一帧与当前帧的差异。这样就可以使用图片识别的数据集进行训练，数据量大大提升。）</p>

<p><img src="/assets/2017-06-05-OSVOS跟进/屏幕快照 2017-06-03 下午12.09.00.png" alt="屏幕快照 2017-06-03 下午12.09.00" /></p>

<p>作者还提出了几种guidence的变体，有box annotation和optical flow</p>

<p>第一张的online finetuning要200次迭代，加上第一张的fintuning平均每帧的预测要12秒</p>

<h2 id="automatic-real-time-background-cut-for-portrait-videos">Automatic Real-time Background Cut for Portrait Videos</h2>

<p>这篇论文是讲怎么从视频里实时抠出人像的，主要是借鉴OSVOS来学习背景。</p>

<p>该网络先学习许多背景的采样，再跟原视频结合，达到更好的消除效果，称为global attenuation</p>

<p><img src="/Users/wth/Desktop/屏幕快照 2017-06-05 上午12.25.38.png" alt="屏幕快照 2017-06-05 上午12.25.38" /></p>

<p>感觉这个问题与video object segmentation差别比较大，因为人的大小基本恒定，而且背景一般是静态的。该网络对于动态背景的表现很差。</p>

<p>启发点可能有对于背景的学习是否可以更重视一些？</p>

<h2 id="deeply-supervised-salient-object-detection-with-short-connections">Deeply Supervised Salient Object Detection with Short Connections</h2>

<p>在HED中，深层的side outputs主要用于定位，浅层的side outputs主要用于表达细节，这启发了作者使用short connections在HED内部构建skip-layer，更好的结合深层与浅层的能力。下图的c和d是作者提出的模型。（以下暂称SCHED(short connected HED)，作者没给官方简称…）</p>

<p><img src="/assets/2017-06-05-OSVOS跟进/屏幕快照 2017-06-04 下午9.59.57.png" alt="屏幕快照 2017-06-04 下午9.59.57" /></p>

<p>这个网络的具体应用我觉得可以有以下几种途径</p>

<ol>
  <li>OSVOS跟进，用这个网络与ImageNet预训练的网络（或者合并成一个预训练过的网络）共同学习如何区分前景和后景，提升OSVOS区分mask的能力，总体步骤不变。
    <ul>
      <li>优势：mask一般是salient object，应该学习起来比较容易，而且SCHED带有轮廓学习能力，可以省略OSVOS中的轮廓CNN，提升速度，简化模型</li>
      <li>劣势：有时候mask是不起眼的物体，比如远处来的赛车，一开始很小，这种情况可能学习起来比较困难</li>
    </ul>
  </li>
  <li>Learning Video Object Segmentation from Static Images跟进，用SCHED代替optical flow，与propagation结合
    <ul>
      <li>优势，更快，轮廓更精确</li>
      <li>劣势，没有明显的理由表明会提升表现</li>
    </ul>
  </li>
</ol>


  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">My blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              My blog
            
            </li>
            
            <li><a href="mailto:phi.wth@gmail.com">phi.wth@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/Wei-TianHao"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">Wei-TianHao</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>随便写写，都是骗人的
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
