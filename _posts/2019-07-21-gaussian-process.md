---
layout: post
title: 高斯过程回归科普
date:   2019-09-24 10:00:00 +0800
tags: bayesian-methods machine-learning
---


> 高斯过程最关键的思想就是，你不想要什么变量，就对这个变量做高斯分布假设...然后就可以计算关于这个变量的边缘分布，把这个变量消掉，建立起其他变量之间的直接联系。

### 简介

高斯过程回归是一类贝叶斯非参方法，解决的问题是给定一堆数据点$(x,y)$后，如何寻找拟合数据最好的函数，同时给出函数的分布，类似于置信区间。

为了引入高斯过程回归，我们先讲一下参数方法，贝叶斯线性回归。

### 贝叶斯线性回归

假设我们现在有一堆数据点$S=(x,y)$，数据的分布满足
$$
y=f(x)+\epsilon,\ \epsilon \sim N(0,\sigma^2)
$$
其中我们假设$f(x)=w^T x$，即线性模型，$\epsilon$是观测噪音。

我们现在的目标是求一个最好的$w$使得$f$尽量好的拟合这些数据点。一个可行的想法是：如果我们知道给定$S$以后$w$的概率分布，那我们就能轻易选出最好的$w$了。即我们的目标是求$w$的后验概率分布$p(w|S)$。

但这个没法直接求，因为逻辑上来说$w$并不取决于你的数据集是什么。所以我们想能不能利用贝叶斯方法，用$p(w)$和$p(S|w)$来求？

$p(w)$可以由我们指定，因为求后验概率的过程就是根据观察调整一个概率分布超参数的过程。我们用$p(w) \sim N(0,\tau^2 I)$，使用高斯分布的原因待会会解释，均值设为$0$是因为我们其实并不知道$w$的分布是怎样的，所以应该尽量少的引入先验知识，而$0$是最无偏的估计。

而$p(S|w)$其实就是$p(y|x,w)$，这两个是完全等价的，因为$(x,y)$是一起给定的，$p(y|x)=1$

现在我们知道了$p(S|w)$和$p(w)$，用贝叶斯公式求$p(w|S)$
$$
p(w|S) = \frac{p(S|w)\ p(w)}{p(S)}\\
p(S)=\int p(w)\ p(S|w)\ dw
$$

这样我们就能选出最适用于当前数据$S$和模型$f$的参数$w$啦！假设我们的测试集是$(x^*,y^*)$，把$w$带进去就可以对$x^*$进行预测。

但贝叶斯方法有个额外的好处，就是不光能给出预测，还能给出预测的分布，即
$$
p(y^*|S,x^*) = \int p(y^*|x^*,w)\ p(w|S)\ dw
$$

一般来说上面两个积分都很不好求，但因为我们的假设分布都是高斯分布，一通很复杂的推导之后我们会发现最后$w|S$和$y^*|S,x^*$都是高斯分布。这就是我们之前为什么全都要假设成高斯分布的原因。


### 高斯过程
上面的方法计算的是某个具体模型的***参数的概率分布***，而高斯过程直接计算所有***模型的概率分布***。因为不管用什么模型，我们总是对于模型有一些假设，这些假设如果符合现实会有很大的帮助，但如果不准（一般都很不准...），则会带来很大的损害。所以我们想能不能直接让算法自己选择模型，即算出模型的概率分布呢？

那怎么才能计算模型的概率分布呢？我们知道函数可以被看做是一个无穷维的向量，我们可以假设每一维（即x上每个点）都是一个随机变量，每个随机变量都符合高斯分布，他们联合起来符合多元高斯分布（无限元高斯分布），即这个函数符合一个多元高斯分布。我们每次观测完，就可以通过最大后验概率调整这个多元高斯分布的参数。观测足够多，就可以得到接近真实的模型概率分布。

这样的话我们要给出一个无穷维的均值向量（一个均值函数）$\mu$和无穷维的协方差矩阵$\sigma$。对于均值函数，我们可以简单粗暴的设成衡为$0$，原因跟之前一样，在没有信息的情况下尽量无偏；无穷维向量我们可以用恒为零的函数，可无穷维的矩阵可怎么办呢？解决方案是，我们假设两个变量的关联程度只跟他们之间的“距离”有关，越近越相关，跟其他的东西都没有关系。设衡量距离的函数为$k(x_i,x_j)$，则协方差中的$(i,j)$项为$k(x_i,x_j)$。给定这些先验后，我们可以说$f$服从这样一个高斯过程:
$$
f(.) \sim GP(0,k(.,.))
$$

但是协方差矩阵不是随便拿数凑一个矩阵就行的，它必须是个半正定的矩阵才行，那$k(.,.)$得满足什么条件才能让协方差一定是半正定矩阵呢？答案就是$k(.,.)$必须得是核函数。其实根据Mercer’s theorem，核函数的定义就是使得组成的矩阵为半正定矩阵的函数。所以，$k(.,.)$是核函数和协方差矩阵半正定互为充要条件。我们这里就不讨论怎么构造核函数了。

这里我们虽然没有假设模型是什么样的，只是给了一个核的先验，但核的先验其实还是隐性的限制了模型的性质。就是说高斯过程也无法探索所有可能的函数形式，只能探索部分一类满足某种条件的函数。但这种限制相比于我们预设模型满足什么形式已经弱多了，因此高斯过程相比于基于参数的方法具有更大的探索空间来寻找最合适的函数。

接下来讲一下实践中怎么用。

实践中要做的就是通过采样，调整模型分布的后验概率，
虽然我们假设了一个无穷维的分布，但在现实中我们的训练数据不是无穷维的，即不可能函数上每个点都采样。我们只有部分变量的采样情况，我们假设有个训练集$(X,\bf{y})$，和测试集$(X_*,\bf{y_*})$。

因为$f$是无穷维的，把$f$的后验分布表示出来既困难又没有必要，我们想要其实只是测试集上预测的分布，即$p(\vec{y}_{*} | \vec{y}, X, X_{*})$，我们可以直接求这个。

$$
y=f(x)+\varepsilon,\ \varepsilon \sim N(0,\sigma^2)
$$
设$\vec{f}, \vec{f^*}$代表$f(.)$在$X$,$X^*$上的输出组成的向量。
$\vec{\varepsilon},\vec{\varepsilon^*}$是噪音在两个数据集上组成的向量。

则有
$$
\left[\begin{array}{c}{\vec{f}} \\ {\vec{f}_{*}}\end{array}\right] \Bigr| X, X_{*} \sim N\left(\overrightarrow{0},\left[\begin{array}{cc}{K(X, X)} & {K\left(X, X_{*}\right)} \\ {K\left(X_{*}, X\right)} & {K\left(X_{*}, X_{*}\right)}\end{array}\right]\right)
$$

$$
{\qquad\left[\begin{array}{c}{\vec{y}} \\ {\vec{y}_{*}}\end{array}\right] | X, X_{*}=\left[\begin{array}{c}{\vec{f}} \\ {\vec{f}_{*}}\end{array}\right]+\left[\begin{array}{c}{\vec{\varepsilon}} \\ {\vec{\varepsilon}_{*}}\end{array}\right] \sim \mathcal{N}\left(\overrightarrow{0},\left[\begin{array}{cc}{K(X, X)+\sigma^{2} I} & {K\left(X, X_{*}\right)} \\ {K\left(X_{*}, X\right)} & {K\left(X_{*}, X_{*}\right)+\sigma^{2} I}\end{array}\right]\right)} \\
$$
我们现在知道了给定$X, X_*$的时候$y, y_*$的概率分布$p(\vec{y}, \vec{y}_{*}\ | X, X_{*})$，一个多元高斯分布。接下来我们想要算出上式的边缘分布，即$p(\vec{y}_{*} | \vec{y}, X, X_{*})$。一般算边缘分布的时候是要求积分的，对上式来说就是求$p(\vec{y}, \vec{y}_{*}\ | X, X_{*})$对$\vec{y}$的积分。但高斯分布实在是太优秀了，我们经过推导以后发现多元高斯的边缘分布有解析解，可以直接写出来，而且还是一个多元高斯分布！推导很复杂，这里就不讲了，感兴趣的可以看一下[这篇博客](https://seanwangjs.github.io/2018/01/08/conditional-gaussian-distribution.html)。大家知道下式是套多元高斯分布的边缘分布公式得到的就行了，最后推出来的边缘分布是
$$
{\qquad \vec{y}_{*} | \vec{y}, X, X_{*} \sim \mathcal{N}\left(\mu^{*}, \Sigma^{*}\right)}
$$
其中
$$
{\qquad \begin{aligned} \mu^{*} &=K\left(X_{*}, X\right)\left(K(X, X)+\sigma^{2} I\right)^{-1} \vec{y} \\ \Sigma^{*} &=K\left(X_{*}, X_{*}\right)+\sigma^{2} I-K\left(X_{*}, X\right)\left(K(X, X)+\sigma^{2} I\right)^{-1} K\left(X, X_{*}\right) \end{aligned}}
$$

这样我们就得到了测试集上预测的分布！$\mu^*$就是我们用后验概率最大的模型在$X_*$上预测出来的结果。

另外提一点，最后的结果直接由输入和先验决定，没有出现任何参数，这也是高斯过程被称为非参方法的原因。但其实我们也可以理解为非参方法实际上就是无穷参方法，因为函数的每一个点都是一个参数。或者我们把最终学到的结果看做是由训练数据作为参数的函数，训练数据越多，参数就越多。


### 高斯过程隐变量模型

高斯过程隐变量模型是一种降维的方法，这种方法假设低维到高维的映射满足一个线性模型，通过假设模型参数符合高斯分布，边缘化了模型中的参数，直接联系起了隐空间和观测空间。感兴趣的可以看一下[这篇博客](https://zhuanlan.zhihu.com/p/30969391)。


### 高斯过程动态模型

高斯过程动态模型就是把高斯过程隐变量模型用于非线性动态系统建模，同样是假设参数满足高斯分布，通过计算边缘分布消掉参数，建立起隐空间和观测空间的联系。


